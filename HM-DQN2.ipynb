{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "#import d2l\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy\n",
    "import yacht_main as yacht\n",
    "import copy\n",
    "import collections\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import IPython as ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Reshape()\n",
       "  (1): Linear(in_features=5, out_features=50, bias=True)\n",
       "  (2): LeakyReLU(negative_slope=0.01)\n",
       "  (3): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (4): LeakyReLU(negative_slope=0.01)\n",
       "  (5): Linear(in_features=30, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_SIZE = 5\n",
    "OUTPUT_SIZE = 12\n",
    "\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(INPUT_SIZE)\n",
    "    \n",
    "net = torch.nn.Sequential(\n",
    "    Reshape(),\n",
    "    \n",
    "    nn.Linear(in_features=INPUT_SIZE, out_features=50),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(50,30),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(30,OUTPUT_SIZE)\n",
    "    \n",
    "    #nn.Linear(in_features=INPUT_SIZE, out_features = OUTPUT_SIZE)\n",
    ")\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.uniform_(m.weight,-0.1,0.1)\n",
    "\n",
    "net = net.float()\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def try_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = try_gpu()\n",
    "print(torch.cuda.is_available())\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data = []\n",
    "SCALE = 30.0\n",
    "\n",
    "\n",
    "def pack(org):\n",
    "    return (org/SCALE)\n",
    "\n",
    "def unpack(norm):\n",
    "    return round(SCALE*float(norm))\n",
    "\n",
    "for d1 in range(6):\n",
    "    for d2 in range(6):\n",
    "        for d3 in range(6):\n",
    "            for d4 in range(6):\n",
    "                for d5 in range(6):\n",
    "                    dices = [d1+1,d2+1,d3+1,d4+1,d5+1]\n",
    "                    scores = [pack(f(dices)) for f in yacht.score_func]\n",
    "                    #scores = [yacht.score_func[6](dices)/SCALE]\n",
    "                    \n",
    "                    dt = torch.tensor([dices.count(i+1) for i in range(6)], dtype = torch.float32, requires_grad = False)\n",
    "                    st = torch.tensor(scores, dtype = torch.float32, requires_grad = False)\n",
    "                    \n",
    "                    train_data.append((dt,st))\n",
    "\n",
    "print(str(len(train_data)))\n",
    "\n",
    "for x,y in train_data:\n",
    "    cont = \"\"\n",
    "    for a in x:\n",
    "        cont += str(int(a))+\" \"\n",
    "    cont += \" | \"\n",
    "    for b in y:\n",
    "        cont += str(unpack(b))+\" \"\n",
    "    print(cont)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): D_Reshape()\n",
       "  (1): Linear(in_features=19, out_features=300, bias=True)\n",
       "  (2): LeakyReLU(negative_slope=0.01)\n",
       "  (3): Linear(in_features=300, out_features=100, bias=True)\n",
       "  (4): LeakyReLU(negative_slope=0.01)\n",
       "  (5): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (6): LeakyReLU(negative_slope=0.01)\n",
       "  (7): Linear(in_features=50, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "MEMORY_SIZE = 10000\n",
    "GAMMA = 0.1\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 500\n",
    "TARGET_UPDATE = 20\n",
    "SCALE = 30.0\n",
    "#ALPHA = 0.1\n",
    "\n",
    "D_INPUT_SIZE = 13+6\n",
    "D_OUTPUT_SIZE = 12+31\n",
    "\n",
    "class D_Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, D_INPUT_SIZE)\n",
    "\n",
    "policy_net = torch.nn.Sequential(\n",
    "    D_Reshape(),\n",
    "    nn.Linear(in_features=D_INPUT_SIZE, out_features=300),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(300,100),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(100,50),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(50,D_OUTPUT_SIZE)\n",
    ")\n",
    "\n",
    "policy_net.apply(init_weights)\n",
    "target_net = copy.deepcopy(policy_net)\n",
    "target_net.eval()\n",
    "\n",
    "policy_net.to(device)\n",
    "target_net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = collections.namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    \n",
    "    \n",
    "memory = ReplayMemory(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRIORITY = 3\n",
    "\n",
    "def train(net, train_iter,criterion, num_epochs, device,lr=None, weight_decay = 0):\n",
    "\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    #optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        #train_l_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        #train_loss_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        train_l_sum = 0.0\n",
    "        train_loss_sum = 0.0\n",
    "        \n",
    "        \n",
    "        n, start = 0, time.time()\n",
    "\n",
    "        for x, y in train_iter:\n",
    "            net.train()\n",
    "            optimizer.zero_grad()\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            \n",
    "            y_hat = net(x)\n",
    "            loss = criterion(y[:7], y_hat[:7]) + PRIORITY * criterion(y[7:],y_hat[7:])\n",
    "            #loss = criterion(torch.atanh(y), torch.atanh(y_hat))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                train_l_sum += loss.float()\n",
    "                train_loss_sum += sum([abs(unpack(d)) for d in (y-y_hat)])\n",
    "                n += 1\n",
    "\n",
    "\n",
    "        print('epoch %d, loss %.4f, train loss %.3f, time %.1f sec, n=%d' %\n",
    "              (epoch + 1, train_l_sum/n*SCALE, train_loss_sum/n, time.time() - start, n) )\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "criterion = nn.MSELoss(reduction = 'sum')\n",
    "train(net, train_data, criterion,1000, device, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(net.state_dict(), './data/HM_learn_scores_' + str(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net.load_state_dict(torch.load('./data/HM_learn_scores_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corr = 0\n",
    "\n",
    "for x,y in train_data:\n",
    "    net.eval()\n",
    "    x,y = x.to(device),y.to(device)\n",
    "    y_hat = net(x)\n",
    "\n",
    "    hy = y.clone().detach()\n",
    "    hyh = y_hat.clone().detach()\n",
    "    \n",
    "    dh = (hy-hyh)\n",
    "    \n",
    "    flag = False\n",
    "\n",
    "    \n",
    "    for i in range(OUTPUT_SIZE):\n",
    "        if not unpack(dh[i]) == 0:\n",
    "            flag = True\n",
    "            break\n",
    "    \n",
    "    if not flag:\n",
    "        corr += 1\n",
    "        continue\n",
    "    \n",
    "    cont = \"\"\n",
    "    for a in x:\n",
    "        cont += str(int(a)) + \" \"\n",
    "    cont += \" | \"\n",
    "    for d in hy-hyh:\n",
    "    #for d in hyh:\n",
    "        cont += str(unpack(d)) + \" \"\n",
    "    print(cont)\n",
    "        \n",
    "print(\"{0}\".format(corr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = [0]\n",
    "def select_action(Qv, is_step = False, with_random = True):\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done[0] / EPS_DECAY)\n",
    "    \n",
    "    if is_step:\n",
    "        steps_done[0] += 1\n",
    "\n",
    "    if not with_random or sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return int(torch.argmax(Qv,1))\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(D_OUTPUT_SIZE)]], device=device, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_scores = []\n",
    "\n",
    "def plot_scores():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    scores_t = torch.tensor(episode_scores, dtype=torch.float32)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.plot(scores_t.numpy())\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(scores_t) >= 10:\n",
    "        means = scores_t.unfold(0, 10, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(9), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    ip.display.clear_output(wait=True)\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_step(pol_net, tar_net, optimizer, criterion):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    #batch = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*memory.sample(BATCH_SIZE)))\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    \n",
    "    Q_state = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    Q_next_max = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    loss = criterion(Q_state, Q_next_max.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \"\"\"loss = 0\n",
    "    \n",
    "    for transition in batch:\n",
    "        Q_state = pol_net(transition.state)\n",
    "        Q_next_max = 0 if transition.next_state == None else torch.max(tar_net(transition.next_state),0)[0]\n",
    "        expected_action_value = Q_next_max * GAMMA + transition.reward\n",
    "        \n",
    "        loss += criterion(Q_state[transition.action].view(1), expected_action_value)\"\"\"\n",
    "        #print(\"{0}<<left | right>>{1}\".format(Q_state[transition.action-31], expected_action_value))\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def D_train(pol_net, tar_net, criterion, num_episodes, device,lr=None, weight_decay = 0, saveidx=0):\n",
    "    \n",
    "    optimizer = optim.SGD(pol_net.parameters(),lr=lr)\n",
    "    pol_net.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        yacht.reset_game()\n",
    "        state, score, _ = yacht.get_yacht_output()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device, requires_grad = False).view(1, len(state))\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            Qv = pol_net(state)\n",
    "            action = select_action(Qv, is_step = True)\n",
    "            reward = yacht.update(action)/SCALE\n",
    "            action = torch.tensor([[action]], device=device, requires_grad = False)\n",
    "\n",
    "            new_state, new_score, done = yacht.get_yacht_output()\n",
    "            step_reward = torch.tensor([float(reward)], device=device, requires_grad = False)\n",
    "\n",
    "            if not done:\n",
    "                new_state = torch.tensor(new_state, dtype=torch.float, device=device, requires_grad = False).view(1, len(new_state))\n",
    "            else:\n",
    "                new_state = None\n",
    "            \n",
    "            memory.push(state,action,new_state,step_reward)\n",
    "\n",
    "            state = new_state\n",
    "            score = new_score\n",
    "\n",
    "\n",
    "            if done:\n",
    "                state, score, _ = yacht.get_yacht_output()\n",
    "                episode_scores.append(score)\n",
    "                #print(\"{0}) {1}\\tscore : {2}, turns = {3}\".format(i_episode, state[:12], score, t+1))\n",
    "\n",
    "                if i_episode % 200 == 0:\n",
    "                    plot_scores()\n",
    "                    #print(\"episode {0}, time elapsed: {1}s\".format(i_episode, round(time.time() - start_time,1)))\n",
    "                    start_time = time.time()\n",
    "\n",
    "                break\n",
    "        \n",
    "        optimize_step(pol_net, tar_net, optimizer,criterion)\n",
    "        optimize_step(pol_net, tar_net, optimizer,criterion)\n",
    "        optimize_step(pol_net, tar_net, optimizer,criterion)\n",
    "        optimize_step(pol_net, tar_net, optimizer,criterion)\n",
    "\n",
    "        #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
    "        \n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            tar_net.load_state_dict(pol_net.state_dict())\n",
    "        if i_episode % 1000 == 999:\n",
    "            saveidx += 1\n",
    "            torch.save(pol_net.state_dict(), './data/HM_DQN2/' + str(saveidx))\n",
    "\n",
    "        \n",
    "        #print(\"episode {0} finished\".format(i_episode))\n",
    "\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'criterion = nn.MSELoss()\\nGAMMA=1\\nD_train(policy_net, target_net, criterion, 500000,device, lr = 0.00001, saveidx = 20000)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''criterion = nn.MSELoss()\n",
    "GAMMA=1\n",
    "D_train(policy_net, target_net, criterion, 500000,device, lr = 0.00001, saveidx = 20000)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-174f9f86c61f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mreal_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mavail_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_value\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0maction_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mavail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreal_action\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mavail_action\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mtotal_fail\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "\n",
    "policy_net.load_state_dict(torch.load('./data/HM_DQN2/20003'))\n",
    "policy_net.to(device)\n",
    "\n",
    "total = 0\n",
    "total_action = 0\n",
    "total_fail = 0\n",
    "for i in range(10000):\n",
    "    yacht.reset_game()\n",
    "    state = []\n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done :\n",
    "        state,score,done = yacht.get_yacht_output()\n",
    "        avail = yacht.get_available_input()\n",
    "        #print(state)\n",
    "        action_value = policy_net(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(1, 19))\n",
    "        \n",
    "        print(avail)\n",
    "        avail = torch.tensor(avail, device = device, dtype=torch.float, requires_grad = False)\n",
    "        \n",
    "        \n",
    "        real_action = action_value.max(1)[1]\n",
    "        avail_action = ((action_value - action_value.min()) * avail).max(1)[1]\n",
    "        if real_action != avail_action:\n",
    "            total_fail+=1\n",
    "        total_action+=1\n",
    "    total += score\n",
    "    if i % 1000 == 0:\n",
    "        print('alarm')\n",
    "    #print(score)\n",
    "print('Final:' + str(total / 10000))\n",
    "print('Fail:' + str(total_fail / total_action))\n",
    "print('FailNum:' + str(total_fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
