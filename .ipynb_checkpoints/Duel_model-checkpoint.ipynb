{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import IPython as ip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import yacht_main as yacht\n",
    "from yacht_test import create_train_set\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(torch.nn.Module):\n",
    "    \"\"\"def __init__(self, size):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.input_size = size\"\"\"\n",
    "    def forward(self, x, *args):\n",
    "        return x.view(args)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.isize = input_size\n",
    "        self.diceonly = nn.Linear(input_size, output_size)\n",
    "        self.dice1 = nn.Linear(input_size, 300)\n",
    "        self.dice2 = nn.Linear(300, 300)\n",
    "        self.dice3 = nn.Linear(300, 50)\n",
    "        \n",
    "        self.diceconv = nn.Conv2d(1, 300, (1, input_size))\n",
    "        self.convshape = Reshape()\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "        #self.dice4 = nn.Linear(100, 100)\n",
    "        #self.dice5 = nn.Linear(100, 100)\n",
    "        #self.dice6 = nn.Linear(100, 100)\n",
    "        self.dice7 = nn.Linear(50, output_size)\n",
    "        #self.score1 = nn.Linear(12, 12)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.convshape(x, -1, 1, 1, self.isize)\n",
    "        #x = self.diceconv(x)\n",
    "        #x = self.convshape(x, -1, 300)\n",
    "        x = self.activation(self.dice1(x))\n",
    "        x = self.activation(self.dice2(x))\n",
    "        x = self.activation(self.dice3(x))\n",
    "        #x = torch.sigmoid(self.dice4(x))\n",
    "        #x = torch.sigmoid(self.dice5(x))\n",
    "        #x = torch.sigmoid(self.dice6(x))\n",
    "        x = self.dice7(x)\n",
    "        #x = self.diceonly(x)\n",
    "        return x\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.constant_(m,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.9\n",
    "EPS_END = 0.5\n",
    "EPS_DECAY = 10000000\n",
    "steps_done = 0\n",
    "INPUT_SIZE = 1\n",
    "OUTPUT_SIZE = 1\n",
    "TARGET_UPDATE = 20\n",
    "TRAINSET_UPDATE = 1000\n",
    "policy_net = None\n",
    "target_net = None\n",
    "\n",
    "def select_action(state, avail):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \"\"\"\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            policy_net(state)\n",
    "            return (policy_net(state) * avail).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(OUTPUT_SIZE)]], device=device, dtype=torch.long)\n",
    "    \"\"\"\n",
    "    if sample <= eps_threshold:\n",
    "        random_action = 0\n",
    "        randomizer = randint(1, avail.sum())\n",
    "        avail_list = torch.reshape(avail,[-1]).tolist()\n",
    "        for i in range(len(avail_list)):\n",
    "            if avail_list[i] == 1:\n",
    "                if randomizer == 1:\n",
    "                    random_action = i\n",
    "                    break\n",
    "                randomizer -= 1\n",
    "        return torch.tensor([[random_action]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            a = policy_net(state)\n",
    "            #a -= a.min()\n",
    "            return a.max(1)[1].view(1, 1)\n",
    "\n",
    "\n",
    "episode_scores = []\n",
    "episode_success = []\n",
    "episode_reward = []\n",
    "\n",
    "def plot_scores():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    scores_t = torch.tensor(episode_scores, dtype=torch.float)\n",
    "    success_t = torch.tensor(episode_success, dtype=torch.float)\n",
    "    reward_t = torch.tensor(episode_reward, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.plot(scores_t.numpy())\n",
    "    if len(scores_t) >= 50:\n",
    "        means = scores_t.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(49), means))\n",
    "        plt.plot(means.numpy())\n",
    "    if len(success_t) >= 50:\n",
    "        sucm = success_t.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        sucm = torch.cat((torch.zeros(49), sucm))\n",
    "        plt.plot(sucm.numpy())\n",
    "    if len(reward_t) >= 50:\n",
    "        rewm = reward_t.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        rewm = torch.cat((torch.zeros(49), rewm))\n",
    "        plt.plot(rewm.numpy())\n",
    "\n",
    "    ip.display.clear_output(wait=True)\n",
    "    plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_net = True\n",
    "def init_net(isize, osize, msize, lr = 0.001, g = 0, reset = False):\n",
    "    global policy_net, target_net, optimizer, memory, EPS_START, EPS_END, EPS_DECAY, steps_done, episode_scores\n",
    "    global TARGET_UPDATE, INPUT_SIZE, OUTPUT_SIZE\n",
    "    \n",
    "    if reset:\n",
    "        INPUT_SIZE = isize\n",
    "        OUTPUT_SIZE = osize\n",
    "\n",
    "        MEMORY_SIZE = msize\n",
    "\n",
    "        TARGET_UPDATE = 5\n",
    "\n",
    "        memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "        policy_net = DQN(INPUT_SIZE,OUTPUT_SIZE).to(device)\n",
    "        target_net = DQN(INPUT_SIZE,OUTPUT_SIZE).to(device)\n",
    "\n",
    "\n",
    "        EPS_START = 0.9\n",
    "        EPS_END = 0.2\n",
    "        EPS_DECAY = 100000\n",
    "        steps_done = 0\n",
    "        GAMMA = g\n",
    "\n",
    "        episode_scores = []\n",
    "        episode_success = []\n",
    "        episode_reward = []\n",
    "    \n",
    "        \n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "    #init_weights(policy_net)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0\n",
    "reward_rate = 0.1\n",
    "regul = 0\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1)) + torch.norm(policy_net(state_batch)) * regul\n",
    "    \"\"\"print('state ex: ' + str(state_batch[0]))\n",
    "    print('action:' + str(action_batch.view(-1)))\n",
    "    print('reward:' + str(reward_batch.view(-1)))\n",
    "    print('value:' + str(state_action_values.view(-1)))\n",
    "    print('expected:' + str(expected_state_action_values.unsqueeze(1).view(-1)))\"\"\"\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #for param in policy_net.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    \"\"\"state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    print('after:' + str(state_action_values.view(-1)))\n",
    "    print('\\n')\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trainset():\n",
    "    print(\"Creating train set...\")\n",
    "    train_set_size = 3000 // 5\n",
    "    train_set = create_train_set(train_set_size)\n",
    "    for state, action, new_state, step_reward in train_set:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float, device=device, requires_grad = False)\n",
    "        action_tensor = torch.tensor([[action - 31]], device=device, dtype=torch.long)\n",
    "        new_state_tensor = torch.tensor(new_state, dtype=torch.float, device=device, requires_grad = False)\n",
    "        step_reward_tensor = torch.tensor([step_reward], device=device, requires_grad = False)\n",
    "        memory.push(state_tensor.reshape(1,INPUT_SIZE), action_tensor, new_state_tensor.reshape(1,INPUT_SIZE), step_reward_tensor)\n",
    "    print(\"Created\", train_set_size * 5, \"train set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def net1_main():\n",
    "    global episode_scores, episode_success, episode_reward\n",
    "    init_net(12, 12, 1000)\n",
    "\n",
    "    num_episodes = 1000000\n",
    "    made_prob = 0.9\n",
    "    episode_scores = []\n",
    "    \n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        reward_total = 0\n",
    "        yacht.reset_game()\n",
    "        state, score, _, avail = yacht.get_yacht_output()\n",
    "        state = torch.tensor(state[:12], dtype=torch.float, device=device, requires_grad = False)\n",
    "        for t in count():\n",
    "            avail = avail[31:]\n",
    "            avail = torch.tensor(avail, dtype=torch.float, device=device, requires_grad = False)\n",
    "            action = select_action(state.reshape(1,INPUT_SIZE), avail) + 31\n",
    "            reward = yacht.update(action)\n",
    "            if reward != -1:\n",
    "                reward_total+=reward\n",
    "\n",
    "            new_state, _, done, avail = yacht.get_yacht_output()\n",
    "            step_reward = torch.tensor([reward], device=device, requires_grad = False)\n",
    "            \n",
    "            #print(str(action) + ' ' + str(reward) + ' ' + str(yacht.dice_status))\n",
    "            \n",
    "            if random.random() < made_prob:\n",
    "                yacht.handled_roll()\n",
    "\n",
    "            if not done:\n",
    "                new_state = torch.tensor(new_state[:12], dtype=torch.float, device=device, requires_grad = False)\n",
    "                memory.push(state.reshape(1,INPUT_SIZE), action - 31, new_state.reshape(1,INPUT_SIZE), \\\n",
    "                        step_reward)\n",
    "            else:\n",
    "                new_state = None\n",
    "                memory.push(state.reshape(1,INPUT_SIZE), action - 31, None, \\\n",
    "                        step_reward)\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "            optimize_model()\n",
    "            if done:\n",
    "                state, score, _, _ = yacht.get_yacht_output()\n",
    "                episode_success.append(state[:12].count(-1) * 10)\n",
    "                episode_scores.append(score)\n",
    "                episode_reward.append(reward_total * 10)\n",
    "                \n",
    "                #print(\"{0}) {1}\\tscore : {2}, turns = {3}\".format(i_episode, state[:12], score, t+1))\n",
    "\n",
    "                if i_episode % 200 == 0:\n",
    "                    plot_scores()\n",
    "\n",
    "                break\n",
    "        #if i_episode % TRAINSET_UPDATE == 0:\n",
    "        #    add_trainset()\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        if i_episode % 10000 == 0:\n",
    "            torch.save(policy_net.state_dict(), './data/net1/net_' + str(i_episode//10000))\n",
    "\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def net2_main(net1_name):\n",
    "    global episode_scores, episode_success\n",
    "    init_net(20, 32, 3000)\n",
    "    \n",
    "    reward_net = DQN(20, 12).to(device)\n",
    "    reward_net.load_state_dict(torch.load('./data/net1/' + net1_name))\n",
    "    reward_net.eval()\n",
    "    reward_net.requires_grad = False\n",
    "    \n",
    "    num_episodes = 500000\n",
    "    made_prob = 0.66\n",
    "    episode_scores = []\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        yacht.reset_game()\n",
    "        state, score, _, avail = yacht.get_yacht_output()\n",
    "        state = torch.tensor(state, dtype=torch.float, device=device, requires_grad = False)\n",
    "        for t in count():\n",
    "            avail = torch.tensor(avail, dtype=torch.float, device=device, requires_grad = False)\n",
    "            action = select_action(state.reshape(1,INPUT_SIZE), avail)\n",
    "            \n",
    "            if action >= 31:\n",
    "                a = policy_net(state)\n",
    "                a -= a.min()\n",
    "                action = (a * avail[31:]).max(1)[1].view(1, 1)\n",
    "            yacht.update(action)\n",
    "\n",
    "            new_state, _, done, avail = yacht.get_yacht_output()\n",
    "            step_reward = reward_net(state).max(1)[0]\n",
    "\n",
    "\n",
    "            if not done:\n",
    "                new_state = torch.tensor(new_state, dtype=torch.float, device=device, requires_grad = False)\n",
    "                memory.push(state.reshape(1,INPUT_SIZE), action, new_state.reshape(1,INPUT_SIZE), \\\n",
    "                        step_reward)\n",
    "            else:\n",
    "                new_state = None\n",
    "                memory.push(state.reshape(1,INPUT_SIZE), action, None, \\\n",
    "                        step_reward)\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "            optimize_model()\n",
    "            if done:\n",
    "                state, score, _, _ = yacht.get_yacht_output()\n",
    "                episode_scores.append(score)\n",
    "                if i_episode % 200 == 0:\n",
    "                    plot_scores()\n",
    "\n",
    "                break\n",
    "\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        if i_episode % 10000 == 0:\n",
    "            torch.save(policy_net.state_dict(), './data/net2/net_' + str(i_episode//10000))\n",
    "\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def net3_main(lr, g):\n",
    "    global episode_scores, episode_success, reset_net\n",
    "    init_net(20, 43, 10000, lr=lr, g=g,reset = reset_net)\n",
    "    reset_net = False\n",
    "    \n",
    "    num_episodes = 50000\n",
    "    num_average = 5\n",
    "    num_optimize = 5\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        episode_memory = []\n",
    "        episode_total = 0\n",
    "        episode_suctotal = 0\n",
    "        for i_average in range(num_average):\n",
    "            yacht.reset_game()\n",
    "            state, score, _, avail = yacht.get_yacht_output()\n",
    "            state = torch.tensor(state, dtype=torch.float, device=device, requires_grad = False)\n",
    "            for t in count():\n",
    "                avail = torch.tensor(avail, dtype=torch.float, device=device, requires_grad = False)\n",
    "                action = select_action(state.reshape(1,INPUT_SIZE), avail)\n",
    "\n",
    "                reward = yacht.update(action)\n",
    "                reward = torch.tensor([reward], device=device, requires_grad = False)\n",
    "\n",
    "                new_state, _, done, avail = yacht.get_yacht_output()\n",
    "\n",
    "                if not done:\n",
    "                    new_state = torch.tensor(new_state, dtype=torch.float, device=device, requires_grad = False)\n",
    "                    episode_memory.append(Transition(state.reshape(1,INPUT_SIZE), action, new_state.reshape(1,INPUT_SIZE), \\\n",
    "                            0))\n",
    "                    state = new_state\n",
    "                else:\n",
    "                    new_state = None\n",
    "                    if reward == -10:\n",
    "                        print('out')\n",
    "                        memory.push(state.reshape(1,INPUT_SIZE), action, None, reward)\n",
    "                    else:\n",
    "                        episode_memory.append(Transition(state.reshape(1,INPUT_SIZE), action, None, 0))                \n",
    "                    state, score, _, _ = yacht.get_yacht_output()\n",
    "                    episode_suctotal += state[:12].count(-1) * 10\n",
    "                    episode_total += score\n",
    "                    break;\n",
    "        \n",
    "        average_reward = episode_total / len(episode_memory)\n",
    "        average_reward = torch.tensor([average_reward], device=device, requires_grad = False)\n",
    "        for x in episode_memory:\n",
    "            memory.push(x.state, x.action, x.next_state, average_reward)\n",
    "        \n",
    "        for i_average in range(num_optimize):\n",
    "            optimize_model()\n",
    "        \n",
    "        \n",
    "        episode_success.append(episode_suctotal/num_average)\n",
    "        episode_scores.append(episode_total/num_average)\n",
    "        if i_episode % 50 == 0:\n",
    "            plot_scores()\n",
    "        #if i_episode % TARGET_UPDATE == 0:\n",
    "        #    target_net.load_state_dict(policy_net.state_dict())\n",
    "        if i_episode % 1000 == 0:\n",
    "            torch.save(policy_net.state_dict(), './data/net3/net_' + str(i_episode//1000))\n",
    "\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def net4_main():\n",
    "    global episode_scores, episode_success\n",
    "    init_net(12, 12, 10000)\n",
    "    \n",
    "    num_episodes = 50000\n",
    "    num_average = 5\n",
    "    num_optimize = 50\n",
    "    made_prob = 0.85\n",
    "    episode_scores = []\n",
    "    episode_success = []\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        episode_memory = []\n",
    "        episode_total = 0\n",
    "        episode_suctotal = 0\n",
    "        for i_average in range(num_average):\n",
    "            \n",
    "            yacht.reset_game()\n",
    "            state, score, _, avail = yacht.get_yacht_output()\n",
    "            state = torch.tensor(state[:12], dtype=torch.float, device=device, requires_grad = False)\n",
    "            for t in count():\n",
    "                avail = avail[31:]\n",
    "                avail = torch.tensor(avail, dtype=torch.float, device=device, requires_grad = False)\n",
    "                action = select_action(state.reshape(1,INPUT_SIZE), avail) + 31\n",
    "                reward = yacht.update(action)\n",
    "                reward = torch.tensor([reward], device=device, requires_grad = False)\n",
    "\n",
    "                new_state, _, done, avail = yacht.get_yacht_output()\n",
    "                new_state = new_state[:12]\n",
    "\n",
    "                if random.random() < made_prob:\n",
    "                    yacht.handled_roll()       \n",
    "                \n",
    "                if not done:\n",
    "                    new_state = torch.tensor(new_state, dtype=torch.float, device=device, requires_grad = False)\n",
    "                    episode_memory.append(Transition(state.reshape(1,INPUT_SIZE), action - 31, new_state.reshape(1,INPUT_SIZE), \\\n",
    "                            0))\n",
    "                    state = new_state\n",
    "                else:\n",
    "                    new_state = None\n",
    "                    if reward == -10:\n",
    "                        memory.push(state.reshape(1,INPUT_SIZE), action - 31, None, reward)\n",
    "                    else:\n",
    "                        episode_memory.append(Transition(state.reshape(1,INPUT_SIZE), action - 31, None, 0))                \n",
    "                    state, score, _, _ = yacht.get_yacht_output()\n",
    "                    episode_suctotal += state[:12].count(-1) * 10\n",
    "                    episode_total += score\n",
    "                    break;\n",
    "        \n",
    "        average_reward = episode_total / len(episode_memory)\n",
    "        average_reward = torch.tensor([average_reward], device=device, requires_grad = False)\n",
    "        for x in episode_memory:\n",
    "            memory.push(x.state, x.action, x.next_state, average_reward)\n",
    "        \n",
    "        for i_average in range(num_optimize):\n",
    "            optimize_model()\n",
    "        \n",
    "        \n",
    "        episode_success.append(episode_suctotal/num_average)\n",
    "        episode_scores.append(episode_total/num_average)\n",
    "        if i_episode % 50 == 0:\n",
    "            plot_scores()\n",
    "        #if i_episode % TARGET_UPDATE == 0:\n",
    "        #    target_net.load_state_dict(policy_net.state_dict())\n",
    "        if i_episode % 1000 == 0:\n",
    "            torch.save(policy_net.state_dict(), './data/net4/net_' + str(i_episode//1000))\n",
    "\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = DQN(20, 12)\n",
    "net2 = DQN(20, 32)\n",
    "def duel_action(state):\n",
    "    action = net2(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(1, 45)).max(1)[1]\n",
    "\n",
    "    if action == 31:\n",
    "        action = net1(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(1, 45)).max(1)[1]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net1r,net2r):\n",
    "    net1 = DQN(20, 12)\n",
    "    net1.load_state_dict(torch.load('./data/net1/' + net1r))\n",
    "    net1.eval()\n",
    "    net2 = DQN(20, 32)\n",
    "    net2.load_state_dict(torch.load('./data/net2/' + net2r))\n",
    "    net2.eval()\n",
    "    \n",
    "    yacht.reset_game()\n",
    "    done = False\n",
    "    while not done :\n",
    "        state,reward,done,_ = yacht.get_yacht_output()\n",
    "        print(state)\n",
    "        action = duel_action(state)\n",
    "        yacht.update(action)\n",
    "        print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net1(net1r):\n",
    "    net1 = DQN(20, 12)\n",
    "    net1.load_state_dict(torch.load('./data/net1/' + net1r))\n",
    "    net1.eval()\n",
    "    \n",
    "    yacht.reset_game()\n",
    "    done = False\n",
    "    while not done :\n",
    "        yacht.handled_roll()\n",
    "        state,reward,done,_ = yacht.get_yacht_output()\n",
    "        print(state)\n",
    "        action = net1(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(1, 45)).max(1)[1] + 31\n",
    "        yacht.update(action)\n",
    "        print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n",
      "out\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e57336d4fc09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet3_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-e378646d3ee9>\u001b[0m in \u001b[0;36mnet3_main\u001b[1;34m(lr, g)\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mINPUT_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myacht\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RoadMageB\\Desktop\\과제\\d2l-pytorch\\COSE474_Deep_Learning\\yacht_main.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(command_index)\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m                 \u001b[0mroll_dice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RoadMageB\\Desktop\\과제\\d2l-pytorch\\COSE474_Deep_Learning\\yacht_main.py\u001b[0m in \u001b[0;36mroll_dice\u001b[1;34m(roll_action_num)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mroll_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mroll_action_num\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mdice_status\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mroll_action_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroll_action_num\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net3_main(0.01, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2_main('net_99')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model('net_', 'net_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net1('net_49')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1r = 'net_7'\n",
    "net1 = DQN(45, 12)\n",
    "net1.load_state_dict(torch.load('./data/net4/' + net1r))\n",
    "net1.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yacht.reset_game()\n",
    "done = False\n",
    "while not done :\n",
    "    yacht.handled_roll()\n",
    "    state,reward,done,_ = yacht.get_yacht_output()\n",
    "    print(state)\n",
    "    print(net1(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(-1)))\n",
    "    action = net1(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(1, 45)).max(1)[1] + 31\n",
    "    reward = yacht.update(action)\n",
    "    print(str(action-30) + ' ' + str(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yacht.reset_game()\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yacht.dice_status = [5, 5, 5, 5, 5]\n",
    "state,reward,done,_ = yacht.get_yacht_output()\n",
    "print(state)\n",
    "print(net1(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(-1)))\n",
    "action = net1(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(1, 45)).max(1)[1] + 31\n",
    "reward = yacht.update(action)\n",
    "print(str(action-30) + ' ' + str(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
