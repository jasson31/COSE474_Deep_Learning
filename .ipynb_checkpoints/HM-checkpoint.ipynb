{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "#import d2l\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy\n",
    "import yacht_main as yacht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Reshape()\n",
       "  (1): Linear(in_features=6, out_features=50, bias=True)\n",
       "  (2): LeakyReLU(negative_slope=0.01)\n",
       "  (3): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (4): LeakyReLU(negative_slope=0.01)\n",
       "  (5): Linear(in_features=30, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_SIZE = 6\n",
    "OUTPUT_SIZE = 12\n",
    "\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(INPUT_SIZE)\n",
    "    \n",
    "net = torch.nn.Sequential(\n",
    "    Reshape(),\n",
    "    \n",
    "    nn.Linear(in_features=INPUT_SIZE, out_features=50),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(50,30),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(30,OUTPUT_SIZE)\n",
    "    \n",
    "    #nn.Linear(in_features=INPUT_SIZE, out_features = OUTPUT_SIZE)\n",
    ")\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.uniform_(m.weight,-0.5,0.5)\n",
    "\n",
    "net = net.float()\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def try_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = try_gpu()\n",
    "print(torch.cuda.is_available())\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n",
      "0 0 0 0 0 5  | 0 0 0 0 0 30 30 30 30 0 0 50 \n",
      "0 0 0 0 1 4  | 0 0 0 0 5 24 29 29 0 0 0 0 \n",
      "0 0 0 0 2 3  | 0 0 0 0 10 18 28 0 28 0 0 0 \n",
      "0 0 0 0 3 2  | 0 0 0 0 15 12 27 0 27 0 0 0 \n",
      "0 0 0 0 4 1  | 0 0 0 0 20 6 26 26 0 0 0 0 \n",
      "0 0 0 0 5 0  | 0 0 0 0 25 0 25 25 25 0 0 50 \n",
      "0 0 0 1 0 4  | 0 0 0 4 0 24 28 28 0 0 0 0 \n",
      "0 0 0 1 1 3  | 0 0 0 4 5 18 27 0 0 0 0 0 \n",
      "0 0 0 1 2 2  | 0 0 0 4 10 12 26 0 0 0 0 0 \n",
      "0 0 0 1 3 1  | 0 0 0 4 15 6 25 0 0 0 0 0 \n",
      "0 0 0 1 4 0  | 0 0 0 4 20 0 24 24 0 0 0 0 \n",
      "0 0 0 2 0 3  | 0 0 0 8 0 18 26 0 26 0 0 0 \n",
      "0 0 0 2 1 2  | 0 0 0 8 5 12 25 0 0 0 0 0 \n",
      "0 0 0 2 2 1  | 0 0 0 8 10 6 24 0 0 0 0 0 \n",
      "0 0 0 2 3 0  | 0 0 0 8 15 0 23 0 23 0 0 0 \n",
      "0 0 0 3 0 2  | 0 0 0 12 0 12 24 0 24 0 0 0 \n",
      "0 0 0 3 1 1  | 0 0 0 12 5 6 23 0 0 0 0 0 \n",
      "0 0 0 3 2 0  | 0 0 0 12 10 0 22 0 22 0 0 0 \n",
      "0 0 0 4 0 1  | 0 0 0 16 0 6 22 22 0 0 0 0 \n",
      "0 0 0 4 1 0  | 0 0 0 16 5 0 21 21 0 0 0 0 \n",
      "0 0 0 5 0 0  | 0 0 0 20 0 0 20 20 20 0 0 50 \n",
      "0 0 1 0 0 4  | 0 0 3 0 0 24 27 27 0 0 0 0 \n",
      "0 0 1 0 1 3  | 0 0 3 0 5 18 26 0 0 0 0 0 \n",
      "0 0 1 0 2 2  | 0 0 3 0 10 12 25 0 0 0 0 0 \n",
      "0 0 1 0 3 1  | 0 0 3 0 15 6 24 0 0 0 0 0 \n",
      "0 0 1 0 4 0  | 0 0 3 0 20 0 23 23 0 0 0 0 \n",
      "0 0 1 1 0 3  | 0 0 3 4 0 18 25 0 0 0 0 0 \n",
      "0 0 1 1 1 2  | 0 0 3 4 5 12 24 0 0 15 0 0 \n",
      "0 0 1 1 2 1  | 0 0 3 4 10 6 23 0 0 15 0 0 \n",
      "0 0 1 1 3 0  | 0 0 3 4 15 0 22 0 0 0 0 0 \n",
      "0 0 1 2 0 2  | 0 0 3 8 0 12 23 0 0 0 0 0 \n",
      "0 0 1 2 1 1  | 0 0 3 8 5 6 22 0 0 15 0 0 \n",
      "0 0 1 2 2 0  | 0 0 3 8 10 0 21 0 0 0 0 0 \n",
      "0 0 1 3 0 1  | 0 0 3 12 0 6 21 0 0 0 0 0 \n",
      "0 0 1 3 1 0  | 0 0 3 12 5 0 20 0 0 0 0 0 \n",
      "0 0 1 4 0 0  | 0 0 3 16 0 0 19 19 0 0 0 0 \n",
      "0 0 2 0 0 3  | 0 0 6 0 0 18 24 0 24 0 0 0 \n",
      "0 0 2 0 1 2  | 0 0 6 0 5 12 23 0 0 0 0 0 \n",
      "0 0 2 0 2 1  | 0 0 6 0 10 6 22 0 0 0 0 0 \n",
      "0 0 2 0 3 0  | 0 0 6 0 15 0 21 0 21 0 0 0 \n",
      "0 0 2 1 0 2  | 0 0 6 4 0 12 22 0 0 0 0 0 \n",
      "0 0 2 1 1 1  | 0 0 6 4 5 6 21 0 0 15 0 0 \n",
      "0 0 2 1 2 0  | 0 0 6 4 10 0 20 0 0 0 0 0 \n",
      "0 0 2 2 0 1  | 0 0 6 8 0 6 20 0 0 0 0 0 \n",
      "0 0 2 2 1 0  | 0 0 6 8 5 0 19 0 0 0 0 0 \n",
      "0 0 2 3 0 0  | 0 0 6 12 0 0 18 0 18 0 0 0 \n",
      "0 0 3 0 0 2  | 0 0 9 0 0 12 21 0 21 0 0 0 \n",
      "0 0 3 0 1 1  | 0 0 9 0 5 6 20 0 0 0 0 0 \n",
      "0 0 3 0 2 0  | 0 0 9 0 10 0 19 0 19 0 0 0 \n",
      "0 0 3 1 0 1  | 0 0 9 4 0 6 19 0 0 0 0 0 \n",
      "0 0 3 1 1 0  | 0 0 9 4 5 0 18 0 0 0 0 0 \n",
      "0 0 3 2 0 0  | 0 0 9 8 0 0 17 0 17 0 0 0 \n",
      "0 0 4 0 0 1  | 0 0 12 0 0 6 18 18 0 0 0 0 \n",
      "0 0 4 0 1 0  | 0 0 12 0 5 0 17 17 0 0 0 0 \n",
      "0 0 4 1 0 0  | 0 0 12 4 0 0 16 16 0 0 0 0 \n",
      "0 0 5 0 0 0  | 0 0 15 0 0 0 15 15 15 0 0 50 \n",
      "0 1 0 0 0 4  | 0 2 0 0 0 24 26 26 0 0 0 0 \n",
      "0 1 0 0 1 3  | 0 2 0 0 5 18 25 0 0 0 0 0 \n",
      "0 1 0 0 2 2  | 0 2 0 0 10 12 24 0 0 0 0 0 \n",
      "0 1 0 0 3 1  | 0 2 0 0 15 6 23 0 0 0 0 0 \n",
      "0 1 0 0 4 0  | 0 2 0 0 20 0 22 22 0 0 0 0 \n",
      "0 1 0 1 0 3  | 0 2 0 4 0 18 24 0 0 0 0 0 \n",
      "0 1 0 1 1 2  | 0 2 0 4 5 12 23 0 0 0 0 0 \n",
      "0 1 0 1 2 1  | 0 2 0 4 10 6 22 0 0 0 0 0 \n",
      "0 1 0 1 3 0  | 0 2 0 4 15 0 21 0 0 0 0 0 \n",
      "0 1 0 2 0 2  | 0 2 0 8 0 12 22 0 0 0 0 0 \n",
      "0 1 0 2 1 1  | 0 2 0 8 5 6 21 0 0 0 0 0 \n",
      "0 1 0 2 2 0  | 0 2 0 8 10 0 20 0 0 0 0 0 \n",
      "0 1 0 3 0 1  | 0 2 0 12 0 6 20 0 0 0 0 0 \n",
      "0 1 0 3 1 0  | 0 2 0 12 5 0 19 0 0 0 0 0 \n",
      "0 1 0 4 0 0  | 0 2 0 16 0 0 18 18 0 0 0 0 \n",
      "0 1 1 0 0 3  | 0 2 3 0 0 18 23 0 0 0 0 0 \n",
      "0 1 1 0 1 2  | 0 2 3 0 5 12 22 0 0 0 0 0 \n",
      "0 1 1 0 2 1  | 0 2 3 0 10 6 21 0 0 0 0 0 \n",
      "0 1 1 0 3 0  | 0 2 3 0 15 0 20 0 0 0 0 0 \n",
      "0 1 1 1 0 2  | 0 2 3 4 0 12 21 0 0 0 0 0 \n",
      "0 1 1 1 1 1  | 0 2 3 4 5 6 20 0 0 15 30 0 \n",
      "0 1 1 1 2 0  | 0 2 3 4 10 0 19 0 0 15 0 0 \n",
      "0 1 1 2 0 1  | 0 2 3 8 0 6 19 0 0 0 0 0 \n",
      "0 1 1 2 1 0  | 0 2 3 8 5 0 18 0 0 15 0 0 \n",
      "0 1 1 3 0 0  | 0 2 3 12 0 0 17 0 0 0 0 0 \n",
      "0 1 2 0 0 2  | 0 2 6 0 0 12 20 0 0 0 0 0 \n",
      "0 1 2 0 1 1  | 0 2 6 0 5 6 19 0 0 0 0 0 \n",
      "0 1 2 0 2 0  | 0 2 6 0 10 0 18 0 0 0 0 0 \n",
      "0 1 2 1 0 1  | 0 2 6 4 0 6 18 0 0 0 0 0 \n",
      "0 1 2 1 1 0  | 0 2 6 4 5 0 17 0 0 15 0 0 \n",
      "0 1 2 2 0 0  | 0 2 6 8 0 0 16 0 0 0 0 0 \n",
      "0 1 3 0 0 1  | 0 2 9 0 0 6 17 0 0 0 0 0 \n",
      "0 1 3 0 1 0  | 0 2 9 0 5 0 16 0 0 0 0 0 \n",
      "0 1 3 1 0 0  | 0 2 9 4 0 0 15 0 0 0 0 0 \n",
      "0 1 4 0 0 0  | 0 2 12 0 0 0 14 14 0 0 0 0 \n",
      "0 2 0 0 0 3  | 0 4 0 0 0 18 22 0 22 0 0 0 \n",
      "0 2 0 0 1 2  | 0 4 0 0 5 12 21 0 0 0 0 0 \n",
      "0 2 0 0 2 1  | 0 4 0 0 10 6 20 0 0 0 0 0 \n",
      "0 2 0 0 3 0  | 0 4 0 0 15 0 19 0 19 0 0 0 \n",
      "0 2 0 1 0 2  | 0 4 0 4 0 12 20 0 0 0 0 0 \n",
      "0 2 0 1 1 1  | 0 4 0 4 5 6 19 0 0 0 0 0 \n",
      "0 2 0 1 2 0  | 0 4 0 4 10 0 18 0 0 0 0 0 \n",
      "0 2 0 2 0 1  | 0 4 0 8 0 6 18 0 0 0 0 0 \n",
      "0 2 0 2 1 0  | 0 4 0 8 5 0 17 0 0 0 0 0 \n",
      "0 2 0 3 0 0  | 0 4 0 12 0 0 16 0 16 0 0 0 \n",
      "0 2 1 0 0 2  | 0 4 3 0 0 12 19 0 0 0 0 0 \n",
      "0 2 1 0 1 1  | 0 4 3 0 5 6 18 0 0 0 0 0 \n",
      "0 2 1 0 2 0  | 0 4 3 0 10 0 17 0 0 0 0 0 \n",
      "0 2 1 1 0 1  | 0 4 3 4 0 6 17 0 0 0 0 0 \n",
      "0 2 1 1 1 0  | 0 4 3 4 5 0 16 0 0 15 0 0 \n",
      "0 2 1 2 0 0  | 0 4 3 8 0 0 15 0 0 0 0 0 \n",
      "0 2 2 0 0 1  | 0 4 6 0 0 6 16 0 0 0 0 0 \n",
      "0 2 2 0 1 0  | 0 4 6 0 5 0 15 0 0 0 0 0 \n",
      "0 2 2 1 0 0  | 0 4 6 4 0 0 14 0 0 0 0 0 \n",
      "0 2 3 0 0 0  | 0 4 9 0 0 0 13 0 13 0 0 0 \n",
      "0 3 0 0 0 2  | 0 6 0 0 0 12 18 0 18 0 0 0 \n",
      "0 3 0 0 1 1  | 0 6 0 0 5 6 17 0 0 0 0 0 \n",
      "0 3 0 0 2 0  | 0 6 0 0 10 0 16 0 16 0 0 0 \n",
      "0 3 0 1 0 1  | 0 6 0 4 0 6 16 0 0 0 0 0 \n",
      "0 3 0 1 1 0  | 0 6 0 4 5 0 15 0 0 0 0 0 \n",
      "0 3 0 2 0 0  | 0 6 0 8 0 0 14 0 14 0 0 0 \n",
      "0 3 1 0 0 1  | 0 6 3 0 0 6 15 0 0 0 0 0 \n",
      "0 3 1 0 1 0  | 0 6 3 0 5 0 14 0 0 0 0 0 \n",
      "0 3 1 1 0 0  | 0 6 3 4 0 0 13 0 0 0 0 0 \n",
      "0 3 2 0 0 0  | 0 6 6 0 0 0 12 0 12 0 0 0 \n",
      "0 4 0 0 0 1  | 0 8 0 0 0 6 14 14 0 0 0 0 \n",
      "0 4 0 0 1 0  | 0 8 0 0 5 0 13 13 0 0 0 0 \n",
      "0 4 0 1 0 0  | 0 8 0 4 0 0 12 12 0 0 0 0 \n",
      "0 4 1 0 0 0  | 0 8 3 0 0 0 11 11 0 0 0 0 \n",
      "0 5 0 0 0 0  | 0 10 0 0 0 0 10 10 10 0 0 50 \n",
      "1 0 0 0 0 4  | 1 0 0 0 0 24 25 25 0 0 0 0 \n",
      "1 0 0 0 1 3  | 1 0 0 0 5 18 24 0 0 0 0 0 \n",
      "1 0 0 0 2 2  | 1 0 0 0 10 12 23 0 0 0 0 0 \n",
      "1 0 0 0 3 1  | 1 0 0 0 15 6 22 0 0 0 0 0 \n",
      "1 0 0 0 4 0  | 1 0 0 0 20 0 21 21 0 0 0 0 \n",
      "1 0 0 1 0 3  | 1 0 0 4 0 18 23 0 0 0 0 0 \n",
      "1 0 0 1 1 2  | 1 0 0 4 5 12 22 0 0 0 0 0 \n",
      "1 0 0 1 2 1  | 1 0 0 4 10 6 21 0 0 0 0 0 \n",
      "1 0 0 1 3 0  | 1 0 0 4 15 0 20 0 0 0 0 0 \n",
      "1 0 0 2 0 2  | 1 0 0 8 0 12 21 0 0 0 0 0 \n",
      "1 0 0 2 1 1  | 1 0 0 8 5 6 20 0 0 0 0 0 \n",
      "1 0 0 2 2 0  | 1 0 0 8 10 0 19 0 0 0 0 0 \n",
      "1 0 0 3 0 1  | 1 0 0 12 0 6 19 0 0 0 0 0 \n",
      "1 0 0 3 1 0  | 1 0 0 12 5 0 18 0 0 0 0 0 \n",
      "1 0 0 4 0 0  | 1 0 0 16 0 0 17 17 0 0 0 0 \n",
      "1 0 1 0 0 3  | 1 0 3 0 0 18 22 0 0 0 0 0 \n",
      "1 0 1 0 1 2  | 1 0 3 0 5 12 21 0 0 0 0 0 \n",
      "1 0 1 0 2 1  | 1 0 3 0 10 6 20 0 0 0 0 0 \n",
      "1 0 1 0 3 0  | 1 0 3 0 15 0 19 0 0 0 0 0 \n",
      "1 0 1 1 0 2  | 1 0 3 4 0 12 20 0 0 0 0 0 \n",
      "1 0 1 1 1 1  | 1 0 3 4 5 6 19 0 0 0 0 0 \n",
      "1 0 1 1 2 0  | 1 0 3 4 10 0 18 0 0 0 0 0 \n",
      "1 0 1 2 0 1  | 1 0 3 8 0 6 18 0 0 0 0 0 \n",
      "1 0 1 2 1 0  | 1 0 3 8 5 0 17 0 0 0 0 0 \n",
      "1 0 1 3 0 0  | 1 0 3 12 0 0 16 0 0 0 0 0 \n",
      "1 0 2 0 0 2  | 1 0 6 0 0 12 19 0 0 0 0 0 \n",
      "1 0 2 0 1 1  | 1 0 6 0 5 6 18 0 0 0 0 0 \n",
      "1 0 2 0 2 0  | 1 0 6 0 10 0 17 0 0 0 0 0 \n",
      "1 0 2 1 0 1  | 1 0 6 4 0 6 17 0 0 0 0 0 \n",
      "1 0 2 1 1 0  | 1 0 6 4 5 0 16 0 0 0 0 0 \n",
      "1 0 2 2 0 0  | 1 0 6 8 0 0 15 0 0 0 0 0 \n",
      "1 0 3 0 0 1  | 1 0 9 0 0 6 16 0 0 0 0 0 \n",
      "1 0 3 0 1 0  | 1 0 9 0 5 0 15 0 0 0 0 0 \n",
      "1 0 3 1 0 0  | 1 0 9 4 0 0 14 0 0 0 0 0 \n",
      "1 0 4 0 0 0  | 1 0 12 0 0 0 13 13 0 0 0 0 \n",
      "1 1 0 0 0 3  | 1 2 0 0 0 18 21 0 0 0 0 0 \n",
      "1 1 0 0 1 2  | 1 2 0 0 5 12 20 0 0 0 0 0 \n",
      "1 1 0 0 2 1  | 1 2 0 0 10 6 19 0 0 0 0 0 \n",
      "1 1 0 0 3 0  | 1 2 0 0 15 0 18 0 0 0 0 0 \n",
      "1 1 0 1 0 2  | 1 2 0 4 0 12 19 0 0 0 0 0 \n",
      "1 1 0 1 1 1  | 1 2 0 4 5 6 18 0 0 0 0 0 \n",
      "1 1 0 1 2 0  | 1 2 0 4 10 0 17 0 0 0 0 0 \n",
      "1 1 0 2 0 1  | 1 2 0 8 0 6 17 0 0 0 0 0 \n",
      "1 1 0 2 1 0  | 1 2 0 8 5 0 16 0 0 0 0 0 \n",
      "1 1 0 3 0 0  | 1 2 0 12 0 0 15 0 0 0 0 0 \n",
      "1 1 1 0 0 2  | 1 2 3 0 0 12 18 0 0 0 0 0 \n",
      "1 1 1 0 1 1  | 1 2 3 0 5 6 17 0 0 0 0 0 \n",
      "1 1 1 0 2 0  | 1 2 3 0 10 0 16 0 0 0 0 0 \n",
      "1 1 1 1 0 1  | 1 2 3 4 0 6 16 0 0 15 0 0 \n",
      "1 1 1 1 1 0  | 1 2 3 4 5 0 15 0 0 15 30 0 \n",
      "1 1 1 2 0 0  | 1 2 3 8 0 0 14 0 0 15 0 0 \n",
      "1 1 2 0 0 1  | 1 2 6 0 0 6 15 0 0 0 0 0 \n",
      "1 1 2 0 1 0  | 1 2 6 0 5 0 14 0 0 0 0 0 \n",
      "1 1 2 1 0 0  | 1 2 6 4 0 0 13 0 0 15 0 0 \n",
      "1 1 3 0 0 0  | 1 2 9 0 0 0 12 0 0 0 0 0 \n",
      "1 2 0 0 0 2  | 1 4 0 0 0 12 17 0 0 0 0 0 \n",
      "1 2 0 0 1 1  | 1 4 0 0 5 6 16 0 0 0 0 0 \n",
      "1 2 0 0 2 0  | 1 4 0 0 10 0 15 0 0 0 0 0 \n",
      "1 2 0 1 0 1  | 1 4 0 4 0 6 15 0 0 0 0 0 \n",
      "1 2 0 1 1 0  | 1 4 0 4 5 0 14 0 0 0 0 0 \n",
      "1 2 0 2 0 0  | 1 4 0 8 0 0 13 0 0 0 0 0 \n",
      "1 2 1 0 0 1  | 1 4 3 0 0 6 14 0 0 0 0 0 \n",
      "1 2 1 0 1 0  | 1 4 3 0 5 0 13 0 0 0 0 0 \n",
      "1 2 1 1 0 0  | 1 4 3 4 0 0 12 0 0 15 0 0 \n",
      "1 2 2 0 0 0  | 1 4 6 0 0 0 11 0 0 0 0 0 \n",
      "1 3 0 0 0 1  | 1 6 0 0 0 6 13 0 0 0 0 0 \n",
      "1 3 0 0 1 0  | 1 6 0 0 5 0 12 0 0 0 0 0 \n",
      "1 3 0 1 0 0  | 1 6 0 4 0 0 11 0 0 0 0 0 \n",
      "1 3 1 0 0 0  | 1 6 3 0 0 0 10 0 0 0 0 0 \n",
      "1 4 0 0 0 0  | 1 8 0 0 0 0 9 9 0 0 0 0 \n",
      "2 0 0 0 0 3  | 2 0 0 0 0 18 20 0 20 0 0 0 \n",
      "2 0 0 0 1 2  | 2 0 0 0 5 12 19 0 0 0 0 0 \n",
      "2 0 0 0 2 1  | 2 0 0 0 10 6 18 0 0 0 0 0 \n",
      "2 0 0 0 3 0  | 2 0 0 0 15 0 17 0 17 0 0 0 \n",
      "2 0 0 1 0 2  | 2 0 0 4 0 12 18 0 0 0 0 0 \n",
      "2 0 0 1 1 1  | 2 0 0 4 5 6 17 0 0 0 0 0 \n",
      "2 0 0 1 2 0  | 2 0 0 4 10 0 16 0 0 0 0 0 \n",
      "2 0 0 2 0 1  | 2 0 0 8 0 6 16 0 0 0 0 0 \n",
      "2 0 0 2 1 0  | 2 0 0 8 5 0 15 0 0 0 0 0 \n",
      "2 0 0 3 0 0  | 2 0 0 12 0 0 14 0 14 0 0 0 \n",
      "2 0 1 0 0 2  | 2 0 3 0 0 12 17 0 0 0 0 0 \n",
      "2 0 1 0 1 1  | 2 0 3 0 5 6 16 0 0 0 0 0 \n",
      "2 0 1 0 2 0  | 2 0 3 0 10 0 15 0 0 0 0 0 \n",
      "2 0 1 1 0 1  | 2 0 3 4 0 6 15 0 0 0 0 0 \n",
      "2 0 1 1 1 0  | 2 0 3 4 5 0 14 0 0 0 0 0 \n",
      "2 0 1 2 0 0  | 2 0 3 8 0 0 13 0 0 0 0 0 \n",
      "2 0 2 0 0 1  | 2 0 6 0 0 6 14 0 0 0 0 0 \n",
      "2 0 2 0 1 0  | 2 0 6 0 5 0 13 0 0 0 0 0 \n",
      "2 0 2 1 0 0  | 2 0 6 4 0 0 12 0 0 0 0 0 \n",
      "2 0 3 0 0 0  | 2 0 9 0 0 0 11 0 11 0 0 0 \n",
      "2 1 0 0 0 2  | 2 2 0 0 0 12 16 0 0 0 0 0 \n",
      "2 1 0 0 1 1  | 2 2 0 0 5 6 15 0 0 0 0 0 \n",
      "2 1 0 0 2 0  | 2 2 0 0 10 0 14 0 0 0 0 0 \n",
      "2 1 0 1 0 1  | 2 2 0 4 0 6 14 0 0 0 0 0 \n",
      "2 1 0 1 1 0  | 2 2 0 4 5 0 13 0 0 0 0 0 \n",
      "2 1 0 2 0 0  | 2 2 0 8 0 0 12 0 0 0 0 0 \n",
      "2 1 1 0 0 1  | 2 2 3 0 0 6 13 0 0 0 0 0 \n",
      "2 1 1 0 1 0  | 2 2 3 0 5 0 12 0 0 0 0 0 \n",
      "2 1 1 1 0 0  | 2 2 3 4 0 0 11 0 0 15 0 0 \n",
      "2 1 2 0 0 0  | 2 2 6 0 0 0 10 0 0 0 0 0 \n",
      "2 2 0 0 0 1  | 2 4 0 0 0 6 12 0 0 0 0 0 \n",
      "2 2 0 0 1 0  | 2 4 0 0 5 0 11 0 0 0 0 0 \n",
      "2 2 0 1 0 0  | 2 4 0 4 0 0 10 0 0 0 0 0 \n",
      "2 2 1 0 0 0  | 2 4 3 0 0 0 9 0 0 0 0 0 \n",
      "2 3 0 0 0 0  | 2 6 0 0 0 0 8 0 8 0 0 0 \n",
      "3 0 0 0 0 2  | 3 0 0 0 0 12 15 0 15 0 0 0 \n",
      "3 0 0 0 1 1  | 3 0 0 0 5 6 14 0 0 0 0 0 \n",
      "3 0 0 0 2 0  | 3 0 0 0 10 0 13 0 13 0 0 0 \n",
      "3 0 0 1 0 1  | 3 0 0 4 0 6 13 0 0 0 0 0 \n",
      "3 0 0 1 1 0  | 3 0 0 4 5 0 12 0 0 0 0 0 \n",
      "3 0 0 2 0 0  | 3 0 0 8 0 0 11 0 11 0 0 0 \n",
      "3 0 1 0 0 1  | 3 0 3 0 0 6 12 0 0 0 0 0 \n",
      "3 0 1 0 1 0  | 3 0 3 0 5 0 11 0 0 0 0 0 \n",
      "3 0 1 1 0 0  | 3 0 3 4 0 0 10 0 0 0 0 0 \n",
      "3 0 2 0 0 0  | 3 0 6 0 0 0 9 0 9 0 0 0 \n",
      "3 1 0 0 0 1  | 3 2 0 0 0 6 11 0 0 0 0 0 \n",
      "3 1 0 0 1 0  | 3 2 0 0 5 0 10 0 0 0 0 0 \n",
      "3 1 0 1 0 0  | 3 2 0 4 0 0 9 0 0 0 0 0 \n",
      "3 1 1 0 0 0  | 3 2 3 0 0 0 8 0 0 0 0 0 \n",
      "3 2 0 0 0 0  | 3 4 0 0 0 0 7 0 7 0 0 0 \n",
      "4 0 0 0 0 1  | 4 0 0 0 0 6 10 10 0 0 0 0 \n",
      "4 0 0 0 1 0  | 4 0 0 0 5 0 9 9 0 0 0 0 \n",
      "4 0 0 1 0 0  | 4 0 0 4 0 0 8 8 0 0 0 0 \n",
      "4 0 1 0 0 0  | 4 0 3 0 0 0 7 7 0 0 0 0 \n",
      "4 1 0 0 0 0  | 4 2 0 0 0 0 6 6 0 0 0 0 \n",
      "5 0 0 0 0 0  | 5 0 0 0 0 0 5 5 5 0 0 50 \n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "SCALE = 30.0\n",
    "\n",
    "\n",
    "def pack(org):\n",
    "    return (org/SCALE)\n",
    "\n",
    "def unpack(norm):\n",
    "    if math.isnan(norm):\n",
    "        return float(100)\n",
    "    return round(SCALE*float(norm))\n",
    "\"\"\"\n",
    "for d1 in range(6):\n",
    "    for d2 in range(6):\n",
    "        for d3 in range(6):\n",
    "            for d4 in range(6):\n",
    "                for d5 in range(6):\n",
    "                    dices = [d1+1,d2+1,d3+1,d4+1,d5+1]\n",
    "                    scores = [pack(f(dices)) for f in yacht.score_func]\n",
    "                    #scores = [yacht.score_func[6](dices)/SCALE]\n",
    "                    \n",
    "                    dt = torch.tensor([dices.count(i+1)/5.0 for i in range(6)], dtype = torch.float32, requires_grad = False)\n",
    "                    st = torch.tensor(scores, dtype = torch.float32, requires_grad = False)\n",
    "                    \n",
    "                    train_data.append((dt,st))\n",
    "\"\"\"\n",
    "\n",
    "for c1 in range(6):\n",
    "    for c2 in range(6-c1):\n",
    "        for c3 in range(6-c1-c2):\n",
    "            for c4 in range(6-c1-c2-c3):\n",
    "                for c5 in range(6-c1-c2-c3-c4):\n",
    "                    c6 = 5-c1-c2-c3-c4-c5\n",
    "                    dices = [1]*c1 + [2]*c2 + [3]*c3 + [4]*c4 + [5]*c5 + [6]*c6\n",
    "\n",
    "                    scores = [pack(f(dices)) for f in yacht.score_func]\n",
    "                    #scores = [yacht.score_func[6](dices)/SCALE]\n",
    "\n",
    "                    dt = torch.tensor([c1,c2,c3,c4,c5,c6], dtype = torch.float32, requires_grad = False)\n",
    "                    st = torch.tensor(scores, dtype = torch.float32, requires_grad = False)\n",
    "\n",
    "                    train_data.append((dt/5.0,st))\n",
    "\n",
    "print(str(len(train_data)))\n",
    "\n",
    "for x,y in train_data:\n",
    "    cont = \"\"\n",
    "    for a in x:\n",
    "        cont += str(int(a*5))+\" \"\n",
    "    cont += \" | \"\n",
    "    for b in y:\n",
    "        cont += str(unpack(b))+\" \"\n",
    "    print(cont)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIORITY = 3\n",
    "saveidx = 0\n",
    "def train(net, train_iter,criterion, num_epochs, device,lr=None, weight_decay = 0):\n",
    "\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    #optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        #train_l_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        #train_loss_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n",
    "        train_l_sum = 0.0\n",
    "        train_loss_sum = 0.0\n",
    "        \n",
    "        \n",
    "        n, start = 0, time.time()\n",
    "\n",
    "        for x, y in train_iter:\n",
    "            net.train()\n",
    "            optimizer.zero_grad()\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            \n",
    "            y_hat = net(x)\n",
    "            loss = criterion(y[:7], y_hat[:7]) + PRIORITY * criterion(y[7:],y_hat[7:])\n",
    "            #loss = criterion(torch.atanh(y), torch.atanh(y_hat))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                train_l_sum += loss.float()\n",
    "                train_loss_sum += sum([abs(unpack(d)) for d in (y-y_hat)])\n",
    "                n += 1\n",
    "\n",
    "        print('epoch %d, loss %.4f, train loss %.3f, time %.1f sec, n=%d' %\n",
    "              (epoch + 1, 100.0*train_l_sum/n*SCALE, train_loss_sum/n, time.time() - start, n) )\n",
    "        \n",
    "    saveidx+=1\n",
    "    torch.save(net.state_dict(), './data/HM_learn_scores_' + str(saveidx))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n",
      "epoch 1, loss 0.0382, train loss 0.575, time 1.0 sec, n=252\n",
      "epoch 2, loss 0.0381, train loss 0.583, time 1.7 sec, n=252\n",
      "epoch 3, loss 0.0380, train loss 0.579, time 1.5 sec, n=252\n",
      "epoch 4, loss 0.0380, train loss 0.583, time 1.9 sec, n=252\n",
      "epoch 5, loss 0.0379, train loss 0.583, time 1.8 sec, n=252\n",
      "epoch 6, loss 0.0379, train loss 0.583, time 1.9 sec, n=252\n",
      "epoch 7, loss 0.0378, train loss 0.583, time 1.8 sec, n=252\n",
      "epoch 8, loss 0.0378, train loss 0.583, time 2.0 sec, n=252\n",
      "epoch 9, loss 0.0378, train loss 0.579, time 1.8 sec, n=252\n",
      "epoch 10, loss 0.0378, train loss 0.579, time 2.1 sec, n=252\n",
      "epoch 11, loss 0.0377, train loss 0.579, time 2.2 sec, n=252\n",
      "epoch 12, loss 0.0377, train loss 0.579, time 2.4 sec, n=252\n",
      "epoch 13, loss 0.0377, train loss 0.579, time 2.4 sec, n=252\n",
      "epoch 14, loss 0.0377, train loss 0.579, time 2.4 sec, n=252\n",
      "epoch 15, loss 0.0377, train loss 0.579, time 2.2 sec, n=252\n",
      "epoch 16, loss 0.0377, train loss 0.579, time 1.6 sec, n=252\n",
      "epoch 17, loss 0.0376, train loss 0.579, time 2.0 sec, n=252\n",
      "epoch 18, loss 0.0377, train loss 0.575, time 1.8 sec, n=252\n",
      "epoch 19, loss 0.0376, train loss 0.575, time 1.8 sec, n=252\n",
      "epoch 20, loss 0.0376, train loss 0.575, time 1.6 sec, n=252\n",
      "epoch 21, loss 0.0376, train loss 0.579, time 1.6 sec, n=252\n",
      "epoch 22, loss 0.0376, train loss 0.579, time 1.7 sec, n=252\n",
      "epoch 23, loss 0.0376, train loss 0.579, time 1.6 sec, n=252\n",
      "epoch 24, loss 0.0376, train loss 0.579, time 1.6 sec, n=252\n",
      "epoch 25, loss 0.0376, train loss 0.579, time 1.7 sec, n=252\n",
      "epoch 26, loss 0.0375, train loss 0.579, time 1.6 sec, n=252\n",
      "epoch 27, loss 0.0375, train loss 0.579, time 1.6 sec, n=252\n",
      "epoch 28, loss 0.0375, train loss 0.579, time 1.6 sec, n=252\n",
      "epoch 29, loss 0.0375, train loss 0.575, time 1.6 sec, n=252\n",
      "epoch 30, loss 0.0375, train loss 0.575, time 1.5 sec, n=252\n",
      "epoch 31, loss 0.0375, train loss 0.575, time 1.6 sec, n=252\n",
      "epoch 32, loss 0.0375, train loss 0.575, time 1.5 sec, n=252\n",
      "epoch 33, loss 0.0375, train loss 0.575, time 1.6 sec, n=252\n",
      "epoch 34, loss 0.0375, train loss 0.579, time 1.0 sec, n=252\n",
      "epoch 35, loss 0.0375, train loss 0.579, time 1.0 sec, n=252\n",
      "epoch 36, loss 0.0375, train loss 0.579, time 1.2 sec, n=252\n",
      "epoch 37, loss 0.0375, train loss 0.579, time 1.1 sec, n=252\n",
      "epoch 38, loss 0.0375, train loss 0.579, time 1.0 sec, n=252\n",
      "epoch 39, loss 0.0375, train loss 0.583, time 0.9 sec, n=252\n",
      "epoch 40, loss 0.0375, train loss 0.579, time 1.1 sec, n=252\n",
      "epoch 41, loss 0.0374, train loss 0.579, time 1.4 sec, n=252\n",
      "epoch 42, loss 0.0374, train loss 0.579, time 1.0 sec, n=252\n",
      "epoch 43, loss 0.0374, train loss 0.579, time 1.0 sec, n=252\n",
      "epoch 44, loss 0.0374, train loss 0.579, time 1.2 sec, n=252\n",
      "epoch 45, loss 0.0374, train loss 0.579, time 1.0 sec, n=252\n",
      "epoch 46, loss 0.0374, train loss 0.579, time 0.8 sec, n=252\n",
      "epoch 47, loss 0.0374, train loss 0.579, time 0.8 sec, n=252\n",
      "epoch 48, loss 0.0374, train loss 0.579, time 0.8 sec, n=252\n",
      "epoch 49, loss 0.0374, train loss 0.579, time 0.8 sec, n=252\n",
      "epoch 50, loss 0.0374, train loss 0.579, time 0.8 sec, n=252\n",
      "epoch 51, loss 0.0374, train loss 0.579, time 0.9 sec, n=252\n",
      "epoch 52, loss 0.0374, train loss 0.583, time 0.9 sec, n=252\n",
      "epoch 53, loss 0.0374, train loss 0.579, time 0.8 sec, n=252\n",
      "epoch 54, loss 0.0374, train loss 0.579, time 0.8 sec, n=252\n",
      "epoch 55, loss 0.0373, train loss 0.579, time 0.9 sec, n=252\n",
      "epoch 56, loss 0.0373, train loss 0.579, time 1.0 sec, n=252\n",
      "epoch 57, loss 0.0373, train loss 0.579, time 0.9 sec, n=252\n",
      "epoch 58, loss 0.0373, train loss 0.579, time 1.2 sec, n=252\n",
      "epoch 59, loss 0.0373, train loss 0.579, time 1.4 sec, n=252\n",
      "epoch 60, loss 0.0373, train loss 0.579, time 1.6 sec, n=252\n",
      "epoch 61, loss 0.0373, train loss 0.579, time 1.8 sec, n=252\n",
      "epoch 62, loss 0.0373, train loss 0.579, time 1.9 sec, n=252\n",
      "epoch 63, loss 0.0373, train loss 0.575, time 1.9 sec, n=252\n",
      "epoch 64, loss 0.0373, train loss 0.579, time 1.9 sec, n=252\n",
      "epoch 65, loss 0.0373, train loss 0.575, time 2.0 sec, n=252\n",
      "epoch 66, loss 0.0373, train loss 0.579, time 2.2 sec, n=252\n",
      "epoch 67, loss 0.0373, train loss 0.575, time 2.1 sec, n=252\n",
      "epoch 68, loss 0.0373, train loss 0.575, time 2.1 sec, n=252\n",
      "epoch 69, loss 0.0373, train loss 0.575, time 2.1 sec, n=252\n",
      "epoch 70, loss 0.0373, train loss 0.575, time 2.1 sec, n=252\n",
      "epoch 71, loss 0.0373, train loss 0.575, time 2.3 sec, n=252\n",
      "epoch 72, loss 0.0373, train loss 0.571, time 2.2 sec, n=252\n",
      "epoch 73, loss 0.0373, train loss 0.575, time 1.6 sec, n=252\n",
      "epoch 74, loss 0.0373, train loss 0.575, time 3.2 sec, n=252\n",
      "epoch 75, loss 0.0373, train loss 0.575, time 3.4 sec, n=252\n",
      "epoch 76, loss 0.0372, train loss 0.571, time 1.6 sec, n=252\n",
      "epoch 77, loss 0.0372, train loss 0.571, time 1.6 sec, n=252\n",
      "epoch 78, loss 0.0372, train loss 0.567, time 1.7 sec, n=252\n",
      "epoch 79, loss 0.0372, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 80, loss 0.0372, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 81, loss 0.0372, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 82, loss 0.0372, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 83, loss 0.0372, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 84, loss 0.0372, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 85, loss 0.0372, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 86, loss 0.0372, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 87, loss 0.0372, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 88, loss 0.0372, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 89, loss 0.0372, train loss 0.567, time 2.4 sec, n=252\n",
      "epoch 90, loss 0.0372, train loss 0.567, time 2.1 sec, n=252\n",
      "epoch 91, loss 0.0372, train loss 0.567, time 2.2 sec, n=252\n",
      "epoch 92, loss 0.0372, train loss 0.567, time 2.1 sec, n=252\n",
      "epoch 93, loss 0.0372, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 94, loss 0.0372, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 95, loss 0.0372, train loss 0.567, time 2.2 sec, n=252\n",
      "epoch 96, loss 0.0372, train loss 0.567, time 2.2 sec, n=252\n",
      "epoch 97, loss 0.0372, train loss 0.567, time 2.2 sec, n=252\n",
      "epoch 98, loss 0.0372, train loss 0.567, time 2.1 sec, n=252\n",
      "epoch 99, loss 0.0371, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 100, loss 0.0371, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 101, loss 0.0371, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 102, loss 0.0371, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 103, loss 0.0371, train loss 0.567, time 2.1 sec, n=252\n",
      "epoch 104, loss 0.0371, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 105, loss 0.0371, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 106, loss 0.0371, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 107, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 108, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 109, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 110, loss 0.0371, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 111, loss 0.0371, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 112, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 113, loss 0.0371, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 114, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 115, loss 0.0371, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 116, loss 0.0371, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 117, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 118, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 119, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 120, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 121, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 122, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 123, loss 0.0371, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 124, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 125, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 126, loss 0.0371, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 127, loss 0.0371, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 128, loss 0.0370, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 129, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 130, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 131, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 132, loss 0.0370, train loss 0.567, time 2.1 sec, n=252\n",
      "epoch 133, loss 0.0370, train loss 0.567, time 2.1 sec, n=252\n",
      "epoch 134, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 135, loss 0.0370, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 136, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 137, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 138, loss 0.0370, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 139, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 140, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 141, loss 0.0370, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 142, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 143, loss 0.0370, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 144, loss 0.0370, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 145, loss 0.0370, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 146, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 147, loss 0.0370, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 148, loss 0.0370, train loss 0.567, time 2.1 sec, n=252\n",
      "epoch 149, loss 0.0370, train loss 0.567, time 2.1 sec, n=252\n",
      "epoch 150, loss 0.0370, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 151, loss 0.0370, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 152, loss 0.0369, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 153, loss 0.0369, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 154, loss 0.0369, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 155, loss 0.0369, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 156, loss 0.0369, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 157, loss 0.0369, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 158, loss 0.0369, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 159, loss 0.0369, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 160, loss 0.0369, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 161, loss 0.0369, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 162, loss 0.0369, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 163, loss 0.0369, train loss 0.567, time 2.1 sec, n=252\n",
      "epoch 164, loss 0.0369, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 165, loss 0.0369, train loss 0.567, time 2.1 sec, n=252\n",
      "epoch 166, loss 0.0369, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 167, loss 0.0369, train loss 0.567, time 2.0 sec, n=252\n",
      "epoch 168, loss 0.0369, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 169, loss 0.0369, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 170, loss 0.0369, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 171, loss 0.0369, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 172, loss 0.0369, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 173, loss 0.0369, train loss 0.567, time 1.8 sec, n=252\n",
      "epoch 174, loss 0.0369, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 175, loss 0.0369, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 176, loss 0.0369, train loss 0.567, time 1.9 sec, n=252\n",
      "epoch 177, loss 0.0369, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 178, loss 0.0369, train loss 0.560, time 2.0 sec, n=252\n",
      "epoch 179, loss 0.0369, train loss 0.563, time 1.9 sec, n=252\n",
      "epoch 180, loss 0.0368, train loss 0.563, time 1.8 sec, n=252\n",
      "epoch 181, loss 0.0369, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 182, loss 0.0368, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 183, loss 0.0368, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 184, loss 0.0368, train loss 0.560, time 1.7 sec, n=252\n",
      "epoch 185, loss 0.0368, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 186, loss 0.0368, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 187, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 188, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 189, loss 0.0368, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 190, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 191, loss 0.0368, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 192, loss 0.0368, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 193, loss 0.0368, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 194, loss 0.0368, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 195, loss 0.0368, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 196, loss 0.0368, train loss 0.556, time 2.1 sec, n=252\n",
      "epoch 197, loss 0.0368, train loss 0.556, time 2.1 sec, n=252\n",
      "epoch 198, loss 0.0368, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 199, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 200, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 201, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 202, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 203, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 204, loss 0.0368, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 205, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 206, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 207, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 208, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 209, loss 0.0368, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 210, loss 0.0368, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 211, loss 0.0368, train loss 0.560, time 2.0 sec, n=252\n",
      "epoch 212, loss 0.0368, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 213, loss 0.0367, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 214, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 215, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 216, loss 0.0367, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 217, loss 0.0367, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 218, loss 0.0367, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 219, loss 0.0367, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 220, loss 0.0367, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 221, loss 0.0367, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 222, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 223, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 224, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 225, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 226, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 227, loss 0.0367, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 228, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 229, loss 0.0367, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 230, loss 0.0367, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 231, loss 0.0367, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 232, loss 0.0367, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 233, loss 0.0367, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 234, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 235, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 236, loss 0.0367, train loss 0.560, time 2.0 sec, n=252\n",
      "epoch 237, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 238, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 239, loss 0.0367, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 240, loss 0.0367, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 241, loss 0.0367, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 242, loss 0.0367, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 243, loss 0.0367, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 244, loss 0.0367, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 245, loss 0.0366, train loss 0.560, time 2.0 sec, n=252\n",
      "epoch 246, loss 0.0367, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 247, loss 0.0366, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 248, loss 0.0366, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 249, loss 0.0366, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 250, loss 0.0366, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 251, loss 0.0366, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 252, loss 0.0366, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 253, loss 0.0366, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 254, loss 0.0366, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 255, loss 0.0366, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 256, loss 0.0366, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 257, loss 0.0366, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 258, loss 0.0366, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 259, loss 0.0366, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 260, loss 0.0366, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 261, loss 0.0366, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 262, loss 0.0366, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 263, loss 0.0366, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 264, loss 0.0366, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 265, loss 0.0366, train loss 0.560, time 2.0 sec, n=252\n",
      "epoch 266, loss 0.0366, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 267, loss 0.0366, train loss 0.556, time 1.8 sec, n=252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 268, loss 0.0366, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 269, loss 0.0366, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 270, loss 0.0366, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 271, loss 0.0366, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 272, loss 0.0366, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 273, loss 0.0366, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 274, loss 0.0366, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 275, loss 0.0366, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 276, loss 0.0366, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 277, loss 0.0365, train loss 0.556, time 2.1 sec, n=252\n",
      "epoch 278, loss 0.0366, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 279, loss 0.0365, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 280, loss 0.0365, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 281, loss 0.0365, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 282, loss 0.0365, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 283, loss 0.0365, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 284, loss 0.0365, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 285, loss 0.0365, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 286, loss 0.0365, train loss 0.556, time 2.1 sec, n=252\n",
      "epoch 287, loss 0.0365, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 288, loss 0.0365, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 289, loss 0.0365, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 290, loss 0.0365, train loss 0.556, time 2.2 sec, n=252\n",
      "epoch 291, loss 0.0365, train loss 0.556, time 1.4 sec, n=252\n",
      "epoch 292, loss 0.0365, train loss 0.556, time 1.5 sec, n=252\n",
      "epoch 293, loss 0.0365, train loss 0.560, time 1.5 sec, n=252\n",
      "epoch 294, loss 0.0365, train loss 0.556, time 1.1 sec, n=252\n",
      "epoch 295, loss 0.0365, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 296, loss 0.0365, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 297, loss 0.0365, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 298, loss 0.0365, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 299, loss 0.0365, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 300, loss 0.0365, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 301, loss 0.0365, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 302, loss 0.0365, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 303, loss 0.0365, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 304, loss 0.0365, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 305, loss 0.0365, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 306, loss 0.0365, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 307, loss 0.0365, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 308, loss 0.0365, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 309, loss 0.0365, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 310, loss 0.0365, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 311, loss 0.0365, train loss 0.556, time 2.1 sec, n=252\n",
      "epoch 312, loss 0.0365, train loss 0.556, time 2.1 sec, n=252\n",
      "epoch 313, loss 0.0364, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 314, loss 0.0364, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 315, loss 0.0364, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 316, loss 0.0364, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 317, loss 0.0364, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 318, loss 0.0364, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 319, loss 0.0364, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 320, loss 0.0364, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 321, loss 0.0364, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 322, loss 0.0364, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 323, loss 0.0364, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 324, loss 0.0364, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 325, loss 0.0364, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 326, loss 0.0364, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 327, loss 0.0364, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 328, loss 0.0364, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 329, loss 0.0364, train loss 0.560, time 1.7 sec, n=252\n",
      "epoch 330, loss 0.0364, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 331, loss 0.0364, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 332, loss 0.0364, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 333, loss 0.0364, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 334, loss 0.0364, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 335, loss 0.0364, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 336, loss 0.0364, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 337, loss 0.0364, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 338, loss 0.0364, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 339, loss 0.0364, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 340, loss 0.0364, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 341, loss 0.0364, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 342, loss 0.0364, train loss 0.556, time 2.2 sec, n=252\n",
      "epoch 343, loss 0.0364, train loss 0.556, time 2.1 sec, n=252\n",
      "epoch 344, loss 0.0364, train loss 0.560, time 2.0 sec, n=252\n",
      "epoch 345, loss 0.0364, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 346, loss 0.0364, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 347, loss 0.0364, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 348, loss 0.0363, train loss 0.560, time 2.0 sec, n=252\n",
      "epoch 349, loss 0.0363, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 350, loss 0.0363, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 351, loss 0.0363, train loss 0.560, time 2.0 sec, n=252\n",
      "epoch 352, loss 0.0363, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 353, loss 0.0363, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 354, loss 0.0363, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 355, loss 0.0363, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 356, loss 0.0363, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 357, loss 0.0363, train loss 0.560, time 1.7 sec, n=252\n",
      "epoch 358, loss 0.0363, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 359, loss 0.0363, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 360, loss 0.0363, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 361, loss 0.0363, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 362, loss 0.0363, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 363, loss 0.0363, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 364, loss 0.0363, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 365, loss 0.0363, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 366, loss 0.0363, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 367, loss 0.0363, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 368, loss 0.0363, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 369, loss 0.0363, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 370, loss 0.0363, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 371, loss 0.0363, train loss 0.560, time 2.1 sec, n=252\n",
      "epoch 372, loss 0.0363, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 373, loss 0.0363, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 374, loss 0.0363, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 375, loss 0.0363, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 376, loss 0.0363, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 377, loss 0.0362, train loss 0.560, time 2.0 sec, n=252\n",
      "epoch 378, loss 0.0363, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 379, loss 0.0363, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 380, loss 0.0363, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 381, loss 0.0363, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 382, loss 0.0363, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 383, loss 0.0363, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 384, loss 0.0363, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 385, loss 0.0363, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 386, loss 0.0362, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 387, loss 0.0362, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 388, loss 0.0362, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 389, loss 0.0362, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 390, loss 0.0362, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 391, loss 0.0362, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 392, loss 0.0362, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 393, loss 0.0362, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 394, loss 0.0362, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 395, loss 0.0362, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 396, loss 0.0362, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 397, loss 0.0362, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 398, loss 0.0362, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 399, loss 0.0362, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 400, loss 0.0362, train loss 0.560, time 1.9 sec, n=252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 401, loss 0.0362, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 402, loss 0.0362, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 403, loss 0.0362, train loss 0.560, time 1.7 sec, n=252\n",
      "epoch 404, loss 0.0362, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 405, loss 0.0362, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 406, loss 0.0362, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 407, loss 0.0362, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 408, loss 0.0362, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 409, loss 0.0362, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 410, loss 0.0362, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 411, loss 0.0362, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 412, loss 0.0362, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 413, loss 0.0362, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 414, loss 0.0362, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 415, loss 0.0362, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 416, loss 0.0362, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 417, loss 0.0362, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 418, loss 0.0361, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 419, loss 0.0362, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 420, loss 0.0362, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 421, loss 0.0362, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 422, loss 0.0361, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 423, loss 0.0361, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 424, loss 0.0362, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 425, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 426, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 427, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 428, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 429, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 430, loss 0.0361, train loss 0.560, time 1.7 sec, n=252\n",
      "epoch 431, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 432, loss 0.0361, train loss 0.560, time 1.7 sec, n=252\n",
      "epoch 433, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 434, loss 0.0361, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 435, loss 0.0361, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 436, loss 0.0361, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 437, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 438, loss 0.0361, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 439, loss 0.0361, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 440, loss 0.0361, train loss 0.560, time 1.7 sec, n=252\n",
      "epoch 441, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 442, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 443, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 444, loss 0.0361, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 445, loss 0.0361, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 446, loss 0.0361, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 447, loss 0.0361, train loss 0.560, time 1.7 sec, n=252\n",
      "epoch 448, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 449, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 450, loss 0.0361, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 451, loss 0.0361, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 452, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 453, loss 0.0361, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 454, loss 0.0360, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 455, loss 0.0361, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 456, loss 0.0361, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 457, loss 0.0361, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 458, loss 0.0361, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 459, loss 0.0361, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 460, loss 0.0360, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 461, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 462, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 463, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 464, loss 0.0360, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 465, loss 0.0361, train loss 0.560, time 2.0 sec, n=252\n",
      "epoch 466, loss 0.0360, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 467, loss 0.0360, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 468, loss 0.0360, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 469, loss 0.0360, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 470, loss 0.0360, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 471, loss 0.0360, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 472, loss 0.0360, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 473, loss 0.0360, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 474, loss 0.0360, train loss 0.560, time 1.7 sec, n=252\n",
      "epoch 475, loss 0.0360, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 476, loss 0.0360, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 477, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 478, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 479, loss 0.0360, train loss 0.563, time 1.8 sec, n=252\n",
      "epoch 480, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 481, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 482, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 483, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 484, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 485, loss 0.0360, train loss 0.563, time 1.9 sec, n=252\n",
      "epoch 486, loss 0.0360, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 487, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 488, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 489, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 490, loss 0.0360, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 491, loss 0.0360, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 492, loss 0.0360, train loss 0.563, time 1.9 sec, n=252\n",
      "epoch 493, loss 0.0360, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 494, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 495, loss 0.0360, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 496, loss 0.0359, train loss 0.563, time 1.8 sec, n=252\n",
      "epoch 497, loss 0.0360, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 498, loss 0.0360, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 499, loss 0.0360, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 500, loss 0.0360, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 501, loss 0.0359, train loss 0.552, time 2.2 sec, n=252\n",
      "epoch 502, loss 0.0359, train loss 0.560, time 2.1 sec, n=252\n",
      "epoch 503, loss 0.0359, train loss 0.556, time 2.2 sec, n=252\n",
      "epoch 504, loss 0.0359, train loss 0.556, time 2.3 sec, n=252\n",
      "epoch 505, loss 0.0359, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 506, loss 0.0359, train loss 0.556, time 1.6 sec, n=252\n",
      "epoch 507, loss 0.0359, train loss 0.560, time 1.1 sec, n=252\n",
      "epoch 508, loss 0.0359, train loss 0.556, time 1.4 sec, n=252\n",
      "epoch 509, loss 0.0359, train loss 0.556, time 4.0 sec, n=252\n",
      "epoch 510, loss 0.0359, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 511, loss 0.0359, train loss 0.556, time 1.1 sec, n=252\n",
      "epoch 512, loss 0.0359, train loss 0.560, time 1.0 sec, n=252\n",
      "epoch 513, loss 0.0359, train loss 0.552, time 1.0 sec, n=252\n",
      "epoch 514, loss 0.0359, train loss 0.556, time 1.0 sec, n=252\n",
      "epoch 515, loss 0.0359, train loss 0.556, time 1.0 sec, n=252\n",
      "epoch 516, loss 0.0359, train loss 0.556, time 1.0 sec, n=252\n",
      "epoch 517, loss 0.0359, train loss 0.556, time 1.0 sec, n=252\n",
      "epoch 518, loss 0.0359, train loss 0.556, time 1.0 sec, n=252\n",
      "epoch 519, loss 0.0359, train loss 0.556, time 1.0 sec, n=252\n",
      "epoch 520, loss 0.0359, train loss 0.556, time 1.1 sec, n=252\n",
      "epoch 521, loss 0.0359, train loss 0.556, time 1.6 sec, n=252\n",
      "epoch 522, loss 0.0359, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 523, loss 0.0359, train loss 0.560, time 2.0 sec, n=252\n",
      "epoch 524, loss 0.0359, train loss 0.556, time 2.3 sec, n=252\n",
      "epoch 525, loss 0.0359, train loss 0.556, time 2.2 sec, n=252\n",
      "epoch 526, loss 0.0359, train loss 0.560, time 2.5 sec, n=252\n",
      "epoch 527, loss 0.0359, train loss 0.556, time 2.6 sec, n=252\n",
      "epoch 528, loss 0.0359, train loss 0.556, time 2.3 sec, n=252\n",
      "epoch 529, loss 0.0359, train loss 0.556, time 2.3 sec, n=252\n",
      "epoch 530, loss 0.0359, train loss 0.556, time 2.6 sec, n=252\n",
      "epoch 531, loss 0.0359, train loss 0.556, time 2.3 sec, n=252\n",
      "epoch 532, loss 0.0359, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 533, loss 0.0359, train loss 0.552, time 1.9 sec, n=252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 534, loss 0.0358, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 535, loss 0.0359, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 536, loss 0.0359, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 537, loss 0.0358, train loss 0.556, time 1.6 sec, n=252\n",
      "epoch 538, loss 0.0359, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 539, loss 0.0359, train loss 0.552, time 1.4 sec, n=252\n",
      "epoch 540, loss 0.0359, train loss 0.556, time 1.2 sec, n=252\n",
      "epoch 541, loss 0.0359, train loss 0.552, time 1.2 sec, n=252\n",
      "epoch 542, loss 0.0358, train loss 0.552, time 1.0 sec, n=252\n",
      "epoch 543, loss 0.0358, train loss 0.552, time 1.0 sec, n=252\n",
      "epoch 544, loss 0.0358, train loss 0.552, time 1.0 sec, n=252\n",
      "epoch 545, loss 0.0358, train loss 0.556, time 1.3 sec, n=252\n",
      "epoch 546, loss 0.0358, train loss 0.552, time 1.0 sec, n=252\n",
      "epoch 547, loss 0.0358, train loss 0.552, time 0.9 sec, n=252\n",
      "epoch 548, loss 0.0358, train loss 0.552, time 0.9 sec, n=252\n",
      "epoch 549, loss 0.0358, train loss 0.552, time 0.9 sec, n=252\n",
      "epoch 550, loss 0.0358, train loss 0.552, time 0.9 sec, n=252\n",
      "epoch 551, loss 0.0358, train loss 0.552, time 0.9 sec, n=252\n",
      "epoch 552, loss 0.0358, train loss 0.552, time 0.9 sec, n=252\n",
      "epoch 553, loss 0.0358, train loss 0.556, time 0.9 sec, n=252\n",
      "epoch 554, loss 0.0358, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 555, loss 0.0358, train loss 0.552, time 0.9 sec, n=252\n",
      "epoch 556, loss 0.0358, train loss 0.552, time 0.9 sec, n=252\n",
      "epoch 557, loss 0.0358, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 558, loss 0.0358, train loss 0.548, time 0.9 sec, n=252\n",
      "epoch 559, loss 0.0358, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 560, loss 0.0358, train loss 0.548, time 0.9 sec, n=252\n",
      "epoch 561, loss 0.0358, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 562, loss 0.0358, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 563, loss 0.0358, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 564, loss 0.0358, train loss 0.548, time 0.9 sec, n=252\n",
      "epoch 565, loss 0.0358, train loss 0.552, time 1.1 sec, n=252\n",
      "epoch 566, loss 0.0358, train loss 0.552, time 1.0 sec, n=252\n",
      "epoch 567, loss 0.0358, train loss 0.548, time 0.9 sec, n=252\n",
      "epoch 568, loss 0.0358, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 569, loss 0.0358, train loss 0.548, time 1.0 sec, n=252\n",
      "epoch 570, loss 0.0358, train loss 0.548, time 1.0 sec, n=252\n",
      "epoch 571, loss 0.0357, train loss 0.552, time 0.9 sec, n=252\n",
      "epoch 572, loss 0.0358, train loss 0.548, time 1.1 sec, n=252\n",
      "epoch 573, loss 0.0358, train loss 0.552, time 0.9 sec, n=252\n",
      "epoch 574, loss 0.0358, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 575, loss 0.0358, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 576, loss 0.0358, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 577, loss 0.0357, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 578, loss 0.0358, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 579, loss 0.0357, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 580, loss 0.0358, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 581, loss 0.0358, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 582, loss 0.0357, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 583, loss 0.0357, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 584, loss 0.0357, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 585, loss 0.0357, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 586, loss 0.0357, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 587, loss 0.0357, train loss 0.548, time 0.7 sec, n=252\n",
      "epoch 588, loss 0.0357, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 589, loss 0.0357, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 590, loss 0.0357, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 591, loss 0.0357, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 592, loss 0.0357, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 593, loss 0.0357, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 594, loss 0.0357, train loss 0.556, time 0.8 sec, n=252\n",
      "epoch 595, loss 0.0357, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 596, loss 0.0357, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 597, loss 0.0357, train loss 0.548, time 0.8 sec, n=252\n",
      "epoch 598, loss 0.0357, train loss 0.552, time 0.9 sec, n=252\n",
      "epoch 599, loss 0.0357, train loss 0.552, time 1.0 sec, n=252\n",
      "epoch 600, loss 0.0357, train loss 0.552, time 0.8 sec, n=252\n",
      "epoch 601, loss 0.0357, train loss 0.552, time 1.0 sec, n=252\n",
      "epoch 602, loss 0.0357, train loss 0.556, time 1.3 sec, n=252\n",
      "epoch 603, loss 0.0357, train loss 0.552, time 1.6 sec, n=252\n",
      "epoch 604, loss 0.0357, train loss 0.552, time 1.7 sec, n=252\n",
      "epoch 605, loss 0.0357, train loss 0.552, time 1.7 sec, n=252\n",
      "epoch 606, loss 0.0357, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 607, loss 0.0357, train loss 0.552, time 1.7 sec, n=252\n",
      "epoch 608, loss 0.0357, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 609, loss 0.0357, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 610, loss 0.0357, train loss 0.552, time 2.1 sec, n=252\n",
      "epoch 611, loss 0.0357, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 612, loss 0.0357, train loss 0.556, time 2.3 sec, n=252\n",
      "epoch 613, loss 0.0357, train loss 0.552, time 2.2 sec, n=252\n",
      "epoch 614, loss 0.0357, train loss 0.552, time 2.2 sec, n=252\n",
      "epoch 615, loss 0.0356, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 616, loss 0.0357, train loss 0.552, time 2.2 sec, n=252\n",
      "epoch 617, loss 0.0357, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 618, loss 0.0357, train loss 0.552, time 2.8 sec, n=252\n",
      "epoch 619, loss 0.0357, train loss 0.552, time 2.9 sec, n=252\n",
      "epoch 620, loss 0.0356, train loss 0.552, time 1.7 sec, n=252\n",
      "epoch 621, loss 0.0357, train loss 0.552, time 1.5 sec, n=252\n",
      "epoch 622, loss 0.0356, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 623, loss 0.0356, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 624, loss 0.0356, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 625, loss 0.0356, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 626, loss 0.0356, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 627, loss 0.0356, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 628, loss 0.0356, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 629, loss 0.0356, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 630, loss 0.0356, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 631, loss 0.0356, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 632, loss 0.0356, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 633, loss 0.0356, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 634, loss 0.0356, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 635, loss 0.0356, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 636, loss 0.0356, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 637, loss 0.0356, train loss 0.556, time 2.1 sec, n=252\n",
      "epoch 638, loss 0.0356, train loss 0.552, time 2.2 sec, n=252\n",
      "epoch 639, loss 0.0356, train loss 0.552, time 2.1 sec, n=252\n",
      "epoch 640, loss 0.0356, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 641, loss 0.0356, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 642, loss 0.0356, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 643, loss 0.0356, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 644, loss 0.0356, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 645, loss 0.0356, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 646, loss 0.0356, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 647, loss 0.0356, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 648, loss 0.0356, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 649, loss 0.0356, train loss 0.556, time 2.0 sec, n=252\n",
      "epoch 650, loss 0.0356, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 651, loss 0.0356, train loss 0.552, time 1.7 sec, n=252\n",
      "epoch 652, loss 0.0356, train loss 0.552, time 1.7 sec, n=252\n",
      "epoch 653, loss 0.0356, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 654, loss 0.0356, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 655, loss 0.0356, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 656, loss 0.0356, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 657, loss 0.0355, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 658, loss 0.0356, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 659, loss 0.0356, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 660, loss 0.0355, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 661, loss 0.0356, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 662, loss 0.0356, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 663, loss 0.0356, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 664, loss 0.0355, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 665, loss 0.0355, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 666, loss 0.0355, train loss 0.552, time 2.0 sec, n=252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 667, loss 0.0355, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 668, loss 0.0355, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 669, loss 0.0355, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 670, loss 0.0355, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 671, loss 0.0355, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 672, loss 0.0355, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 673, loss 0.0355, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 674, loss 0.0355, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 675, loss 0.0355, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 676, loss 0.0355, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 677, loss 0.0355, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 678, loss 0.0355, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 679, loss 0.0355, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 680, loss 0.0355, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 681, loss 0.0355, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 682, loss 0.0355, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 683, loss 0.0355, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 684, loss 0.0355, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 685, loss 0.0355, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 686, loss 0.0355, train loss 0.552, time 1.7 sec, n=252\n",
      "epoch 687, loss 0.0355, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 688, loss 0.0355, train loss 0.560, time 1.7 sec, n=252\n",
      "epoch 689, loss 0.0355, train loss 0.552, time 1.7 sec, n=252\n",
      "epoch 690, loss 0.0355, train loss 0.552, time 1.7 sec, n=252\n",
      "epoch 691, loss 0.0355, train loss 0.552, time 1.7 sec, n=252\n",
      "epoch 692, loss 0.0355, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 693, loss 0.0355, train loss 0.552, time 1.7 sec, n=252\n",
      "epoch 694, loss 0.0355, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 695, loss 0.0355, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 696, loss 0.0355, train loss 0.552, time 2.0 sec, n=252\n",
      "epoch 697, loss 0.0354, train loss 0.556, time 2.1 sec, n=252\n",
      "epoch 698, loss 0.0355, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 699, loss 0.0355, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 700, loss 0.0355, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 701, loss 0.0355, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 702, loss 0.0355, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 703, loss 0.0354, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 704, loss 0.0355, train loss 0.552, time 1.8 sec, n=252\n",
      "epoch 705, loss 0.0354, train loss 0.560, time 1.8 sec, n=252\n",
      "epoch 706, loss 0.0354, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 707, loss 0.0354, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 708, loss 0.0354, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 709, loss 0.0354, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 710, loss 0.0354, train loss 0.560, time 1.9 sec, n=252\n",
      "epoch 711, loss 0.0354, train loss 0.552, time 1.9 sec, n=252\n",
      "epoch 712, loss 0.0354, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 713, loss 0.0354, train loss 0.556, time 1.9 sec, n=252\n",
      "epoch 714, loss 0.0369, train loss 0.563, time 1.9 sec, n=252\n",
      "epoch 715, loss 0.0363, train loss 0.556, time 1.8 sec, n=252\n",
      "epoch 716, loss 0.0360, train loss 0.556, time 1.6 sec, n=252\n",
      "epoch 717, loss 0.0357, train loss 0.556, time 1.7 sec, n=252\n",
      "epoch 718, loss 0.0355, train loss 0.548, time 1.8 sec, n=252\n",
      "epoch 719, loss 0.0354, train loss 0.548, time 1.8 sec, n=252\n",
      "epoch 720, loss 0.0353, train loss 0.548, time 1.9 sec, n=252\n",
      "epoch 721, loss 0.0352, train loss 0.544, time 1.9 sec, n=252\n",
      "epoch 722, loss 0.0351, train loss 0.544, time 1.8 sec, n=252\n",
      "epoch 723, loss 0.0350, train loss 0.544, time 1.8 sec, n=252\n",
      "epoch 724, loss 0.0349, train loss 0.544, time 1.7 sec, n=252\n",
      "epoch 725, loss 0.0348, train loss 0.544, time 1.8 sec, n=252\n",
      "epoch 726, loss 0.0348, train loss 0.544, time 1.9 sec, n=252\n",
      "epoch 727, loss 0.0347, train loss 0.544, time 1.8 sec, n=252\n",
      "epoch 728, loss 0.0346, train loss 0.544, time 1.9 sec, n=252\n",
      "epoch 729, loss 0.0345, train loss 0.544, time 1.8 sec, n=252\n",
      "epoch 730, loss 0.0345, train loss 0.544, time 1.8 sec, n=252\n",
      "epoch 731, loss 0.0345, train loss 0.544, time 1.8 sec, n=252\n",
      "epoch 732, loss 0.0344, train loss 0.544, time 1.8 sec, n=252\n",
      "epoch 733, loss 0.0343, train loss 0.540, time 1.8 sec, n=252\n",
      "epoch 734, loss 0.0344, train loss 0.540, time 1.7 sec, n=252\n",
      "epoch 735, loss 0.0343, train loss 0.544, time 1.7 sec, n=252\n",
      "epoch 736, loss 0.0343, train loss 0.540, time 1.8 sec, n=252\n",
      "epoch 737, loss 0.0341, train loss 0.540, time 1.8 sec, n=252\n",
      "epoch 738, loss 0.0341, train loss 0.544, time 1.8 sec, n=252\n",
      "epoch 739, loss 0.0342, train loss 0.540, time 1.9 sec, n=252\n",
      "epoch 740, loss 0.0341, train loss 0.540, time 1.7 sec, n=252\n",
      "epoch 741, loss 0.0341, train loss 0.540, time 1.7 sec, n=252\n",
      "epoch 742, loss 0.0339, train loss 0.548, time 1.7 sec, n=252\n",
      "epoch 743, loss 0.0340, train loss 0.536, time 1.8 sec, n=252\n",
      "epoch 744, loss 0.0339, train loss 0.536, time 1.9 sec, n=252\n",
      "epoch 745, loss 0.0338, train loss 0.540, time 2.0 sec, n=252\n",
      "epoch 746, loss 0.0338, train loss 0.540, time 2.0 sec, n=252\n",
      "epoch 747, loss 0.0338, train loss 0.536, time 1.8 sec, n=252\n",
      "epoch 748, loss 0.0338, train loss 0.540, time 1.7 sec, n=252\n",
      "epoch 749, loss 0.0337, train loss 0.536, time 1.7 sec, n=252\n",
      "epoch 750, loss 0.0337, train loss 0.540, time 1.8 sec, n=252\n",
      "epoch 751, loss 0.0337, train loss 0.536, time 1.7 sec, n=252\n",
      "epoch 752, loss 0.0336, train loss 0.536, time 1.7 sec, n=252\n",
      "epoch 753, loss 0.0336, train loss 0.536, time 1.7 sec, n=252\n",
      "epoch 754, loss 0.0336, train loss 0.540, time 1.8 sec, n=252\n",
      "epoch 755, loss 0.0335, train loss 0.536, time 1.8 sec, n=252\n",
      "epoch 756, loss 0.0335, train loss 0.536, time 1.8 sec, n=252\n",
      "epoch 757, loss 0.0335, train loss 0.540, time 1.8 sec, n=252\n",
      "epoch 758, loss 0.0334, train loss 0.536, time 1.9 sec, n=252\n",
      "epoch 759, loss 0.0334, train loss 0.532, time 1.8 sec, n=252\n",
      "epoch 760, loss 0.0334, train loss 0.540, time 1.9 sec, n=252\n",
      "epoch 761, loss 0.0334, train loss 0.532, time 1.8 sec, n=252\n",
      "epoch 762, loss 0.0333, train loss 0.532, time 1.8 sec, n=252\n",
      "epoch 763, loss 0.0333, train loss 0.540, time 1.8 sec, n=252\n",
      "epoch 764, loss 0.0333, train loss 0.532, time 1.8 sec, n=252\n",
      "epoch 765, loss 0.0332, train loss 0.532, time 1.8 sec, n=252\n",
      "epoch 766, loss 0.0333, train loss 0.536, time 1.9 sec, n=252\n",
      "epoch 767, loss 0.0332, train loss 0.532, time 1.8 sec, n=252\n",
      "epoch 768, loss 0.0332, train loss 0.532, time 1.8 sec, n=252\n",
      "epoch 769, loss 0.0332, train loss 0.528, time 1.8 sec, n=252\n",
      "epoch 770, loss 0.0332, train loss 0.528, time 1.7 sec, n=252\n",
      "epoch 771, loss 0.0331, train loss 0.528, time 1.8 sec, n=252\n",
      "epoch 772, loss 0.0331, train loss 0.528, time 1.8 sec, n=252\n",
      "epoch 773, loss 0.0331, train loss 0.528, time 1.7 sec, n=252\n",
      "epoch 774, loss 0.0331, train loss 0.528, time 1.8 sec, n=252\n",
      "epoch 775, loss 0.0330, train loss 0.528, time 1.8 sec, n=252\n",
      "epoch 776, loss 0.0330, train loss 0.528, time 2.0 sec, n=252\n",
      "epoch 777, loss 0.0330, train loss 0.528, time 1.8 sec, n=252\n",
      "epoch 778, loss 0.0330, train loss 0.532, time 1.8 sec, n=252\n",
      "epoch 779, loss 0.0330, train loss 0.524, time 1.8 sec, n=252\n",
      "epoch 780, loss 0.0330, train loss 0.528, time 1.8 sec, n=252\n",
      "epoch 781, loss 0.0329, train loss 0.524, time 1.8 sec, n=252\n",
      "epoch 782, loss 0.0329, train loss 0.524, time 1.8 sec, n=252\n",
      "epoch 783, loss 0.0329, train loss 0.524, time 1.9 sec, n=252\n",
      "epoch 784, loss 0.0329, train loss 0.524, time 1.9 sec, n=252\n",
      "epoch 785, loss 0.0329, train loss 0.524, time 2.0 sec, n=252\n",
      "epoch 786, loss 0.0328, train loss 0.520, time 1.9 sec, n=252\n",
      "epoch 787, loss 0.0328, train loss 0.524, time 1.9 sec, n=252\n",
      "epoch 788, loss 0.0328, train loss 0.524, time 1.9 sec, n=252\n",
      "epoch 789, loss 0.0328, train loss 0.520, time 1.8 sec, n=252\n",
      "epoch 790, loss 0.0328, train loss 0.520, time 1.9 sec, n=252\n",
      "epoch 791, loss 0.0328, train loss 0.524, time 1.8 sec, n=252\n",
      "epoch 792, loss 0.0328, train loss 0.524, time 1.9 sec, n=252\n",
      "epoch 793, loss 0.0327, train loss 0.520, time 1.9 sec, n=252\n",
      "epoch 794, loss 0.0327, train loss 0.520, time 2.1 sec, n=252\n",
      "epoch 795, loss 0.0327, train loss 0.520, time 2.0 sec, n=252\n",
      "epoch 796, loss 0.0327, train loss 0.520, time 1.8 sec, n=252\n",
      "epoch 797, loss 0.0327, train loss 0.520, time 1.7 sec, n=252\n",
      "epoch 798, loss 0.0327, train loss 0.520, time 1.7 sec, n=252\n",
      "epoch 799, loss 0.0326, train loss 0.524, time 1.8 sec, n=252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 800, loss 0.0326, train loss 0.520, time 1.8 sec, n=252\n",
      "epoch 801, loss 0.0326, train loss 0.520, time 1.8 sec, n=252\n",
      "epoch 802, loss 0.0326, train loss 0.524, time 1.8 sec, n=252\n",
      "epoch 803, loss 0.0326, train loss 0.516, time 1.7 sec, n=252\n",
      "epoch 804, loss 0.0326, train loss 0.516, time 1.7 sec, n=252\n",
      "epoch 805, loss 0.0326, train loss 0.516, time 1.7 sec, n=252\n",
      "epoch 806, loss 0.0326, train loss 0.520, time 1.7 sec, n=252\n",
      "epoch 807, loss 0.0325, train loss 0.520, time 1.8 sec, n=252\n",
      "epoch 808, loss 0.0326, train loss 0.524, time 1.8 sec, n=252\n",
      "epoch 809, loss 0.0325, train loss 0.520, time 1.9 sec, n=252\n",
      "epoch 810, loss 0.0325, train loss 0.516, time 1.8 sec, n=252\n",
      "epoch 811, loss 0.0325, train loss 0.520, time 1.8 sec, n=252\n",
      "epoch 812, loss 0.0325, train loss 0.520, time 1.7 sec, n=252\n",
      "epoch 813, loss 0.0325, train loss 0.516, time 1.7 sec, n=252\n",
      "epoch 814, loss 0.0325, train loss 0.520, time 1.8 sec, n=252\n",
      "epoch 815, loss 0.0325, train loss 0.516, time 1.9 sec, n=252\n",
      "epoch 816, loss 0.0325, train loss 0.516, time 1.8 sec, n=252\n",
      "epoch 817, loss 0.0325, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 818, loss 0.0324, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 819, loss 0.0324, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 820, loss 0.0324, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 821, loss 0.0324, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 822, loss 0.0324, train loss 0.516, time 1.9 sec, n=252\n",
      "epoch 823, loss 0.0324, train loss 0.516, time 1.9 sec, n=252\n",
      "epoch 824, loss 0.0324, train loss 0.520, time 1.9 sec, n=252\n",
      "epoch 825, loss 0.0323, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 826, loss 0.0324, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 827, loss 0.0323, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 828, loss 0.0323, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 829, loss 0.0323, train loss 0.516, time 1.7 sec, n=252\n",
      "epoch 830, loss 0.0323, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 831, loss 0.0323, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 832, loss 0.0323, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 833, loss 0.0323, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 834, loss 0.0323, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 835, loss 0.0323, train loss 0.516, time 1.7 sec, n=252\n",
      "epoch 836, loss 0.0323, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 837, loss 0.0322, train loss 0.512, time 2.0 sec, n=252\n",
      "epoch 838, loss 0.0323, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 839, loss 0.0322, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 840, loss 0.0322, train loss 0.512, time 2.0 sec, n=252\n",
      "epoch 841, loss 0.0322, train loss 0.516, time 1.8 sec, n=252\n",
      "epoch 842, loss 0.0322, train loss 0.512, time 2.0 sec, n=252\n",
      "epoch 843, loss 0.0322, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 844, loss 0.0322, train loss 0.512, time 2.0 sec, n=252\n",
      "epoch 845, loss 0.0322, train loss 0.512, time 2.0 sec, n=252\n",
      "epoch 846, loss 0.0322, train loss 0.512, time 2.0 sec, n=252\n",
      "epoch 847, loss 0.0321, train loss 0.512, time 2.1 sec, n=252\n",
      "epoch 848, loss 0.0322, train loss 0.512, time 2.0 sec, n=252\n",
      "epoch 849, loss 0.0321, train loss 0.512, time 2.3 sec, n=252\n",
      "epoch 850, loss 0.0321, train loss 0.512, time 2.0 sec, n=252\n",
      "epoch 851, loss 0.0321, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 852, loss 0.0321, train loss 0.512, time 2.0 sec, n=252\n",
      "epoch 853, loss 0.0321, train loss 0.512, time 2.2 sec, n=252\n",
      "epoch 854, loss 0.0321, train loss 0.516, time 2.2 sec, n=252\n",
      "epoch 855, loss 0.0321, train loss 0.512, time 2.2 sec, n=252\n",
      "epoch 856, loss 0.0321, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 857, loss 0.0321, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 858, loss 0.0321, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 859, loss 0.0321, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 860, loss 0.0321, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 861, loss 0.0320, train loss 0.512, time 1.6 sec, n=252\n",
      "epoch 862, loss 0.0321, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 863, loss 0.0320, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 864, loss 0.0320, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 865, loss 0.0321, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 866, loss 0.0320, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 867, loss 0.0320, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 868, loss 0.0320, train loss 0.512, time 1.6 sec, n=252\n",
      "epoch 869, loss 0.0320, train loss 0.512, time 1.6 sec, n=252\n",
      "epoch 870, loss 0.0320, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 871, loss 0.0320, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 872, loss 0.0320, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 873, loss 0.0320, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 874, loss 0.0320, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 875, loss 0.0319, train loss 0.508, time 1.9 sec, n=252\n",
      "epoch 876, loss 0.0320, train loss 0.508, time 2.0 sec, n=252\n",
      "epoch 877, loss 0.0319, train loss 0.512, time 2.0 sec, n=252\n",
      "epoch 878, loss 0.0319, train loss 0.512, time 2.0 sec, n=252\n",
      "epoch 879, loss 0.0319, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 880, loss 0.0319, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 881, loss 0.0319, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 882, loss 0.0319, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 883, loss 0.0319, train loss 0.504, time 1.7 sec, n=252\n",
      "epoch 884, loss 0.0319, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 885, loss 0.0319, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 886, loss 0.0319, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 887, loss 0.0319, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 888, loss 0.0319, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 889, loss 0.0319, train loss 0.512, time 1.8 sec, n=252\n",
      "epoch 890, loss 0.0319, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 891, loss 0.0318, train loss 0.508, time 1.9 sec, n=252\n",
      "epoch 892, loss 0.0319, train loss 0.512, time 1.9 sec, n=252\n",
      "epoch 893, loss 0.0319, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 894, loss 0.0318, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 895, loss 0.0318, train loss 0.508, time 1.6 sec, n=252\n",
      "epoch 896, loss 0.0318, train loss 0.512, time 1.6 sec, n=252\n",
      "epoch 897, loss 0.0318, train loss 0.508, time 1.6 sec, n=252\n",
      "epoch 898, loss 0.0318, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 899, loss 0.0318, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 900, loss 0.0318, train loss 0.512, time 1.7 sec, n=252\n",
      "epoch 901, loss 0.0318, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 902, loss 0.0318, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 903, loss 0.0318, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 904, loss 0.0318, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 905, loss 0.0318, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 906, loss 0.0318, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 907, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 908, loss 0.0318, train loss 0.508, time 1.9 sec, n=252\n",
      "epoch 909, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 910, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 911, loss 0.0318, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 912, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 913, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 914, loss 0.0317, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 915, loss 0.0317, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 916, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 917, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 918, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 919, loss 0.0317, train loss 0.508, time 1.9 sec, n=252\n",
      "epoch 920, loss 0.0317, train loss 0.508, time 1.9 sec, n=252\n",
      "epoch 921, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 922, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 923, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 924, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 925, loss 0.0317, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 926, loss 0.0316, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 927, loss 0.0317, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 928, loss 0.0316, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 929, loss 0.0316, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 930, loss 0.0316, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 931, loss 0.0316, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 932, loss 0.0316, train loss 0.508, time 1.8 sec, n=252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 933, loss 0.0316, train loss 0.508, time 1.9 sec, n=252\n",
      "epoch 934, loss 0.0316, train loss 0.508, time 2.0 sec, n=252\n",
      "epoch 935, loss 0.0316, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 936, loss 0.0316, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 937, loss 0.0316, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 938, loss 0.0316, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 939, loss 0.0316, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 940, loss 0.0316, train loss 0.508, time 1.9 sec, n=252\n",
      "epoch 941, loss 0.0316, train loss 0.508, time 1.9 sec, n=252\n",
      "epoch 942, loss 0.0316, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 943, loss 0.0316, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 944, loss 0.0316, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 945, loss 0.0315, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 946, loss 0.0316, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 947, loss 0.0315, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 948, loss 0.0315, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 949, loss 0.0315, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 950, loss 0.0316, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 951, loss 0.0315, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 952, loss 0.0315, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 953, loss 0.0315, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 954, loss 0.0315, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 955, loss 0.0315, train loss 0.508, time 1.7 sec, n=252\n",
      "epoch 956, loss 0.0315, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 957, loss 0.0315, train loss 0.508, time 1.9 sec, n=252\n",
      "epoch 958, loss 0.0315, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 959, loss 0.0315, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 960, loss 0.0315, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 961, loss 0.0315, train loss 0.508, time 1.9 sec, n=252\n",
      "epoch 962, loss 0.0315, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 963, loss 0.0315, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 964, loss 0.0315, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 965, loss 0.0315, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 966, loss 0.0315, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 967, loss 0.0315, train loss 0.504, time 1.9 sec, n=252\n",
      "epoch 968, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 969, loss 0.0314, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 970, loss 0.0314, train loss 0.504, time 1.7 sec, n=252\n",
      "epoch 971, loss 0.0314, train loss 0.504, time 1.7 sec, n=252\n",
      "epoch 972, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 973, loss 0.0314, train loss 0.504, time 1.9 sec, n=252\n",
      "epoch 974, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 975, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 976, loss 0.0314, train loss 0.504, time 1.9 sec, n=252\n",
      "epoch 977, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 978, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 979, loss 0.0314, train loss 0.504, time 1.9 sec, n=252\n",
      "epoch 980, loss 0.0314, train loss 0.504, time 1.9 sec, n=252\n",
      "epoch 981, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 982, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 983, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 984, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 985, loss 0.0314, train loss 0.504, time 1.9 sec, n=252\n",
      "epoch 986, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 987, loss 0.0314, train loss 0.500, time 1.8 sec, n=252\n",
      "epoch 988, loss 0.0313, train loss 0.504, time 1.9 sec, n=252\n",
      "epoch 989, loss 0.0314, train loss 0.508, time 2.0 sec, n=252\n",
      "epoch 990, loss 0.0313, train loss 0.504, time 2.1 sec, n=252\n",
      "epoch 991, loss 0.0314, train loss 0.504, time 1.8 sec, n=252\n",
      "epoch 992, loss 0.0313, train loss 0.504, time 1.9 sec, n=252\n",
      "epoch 993, loss 0.0313, train loss 0.504, time 1.9 sec, n=252\n",
      "epoch 994, loss 0.0313, train loss 0.500, time 1.9 sec, n=252\n",
      "epoch 995, loss 0.0313, train loss 0.500, time 1.9 sec, n=252\n",
      "epoch 996, loss 0.0313, train loss 0.500, time 1.8 sec, n=252\n",
      "epoch 997, loss 0.0313, train loss 0.508, time 1.8 sec, n=252\n",
      "epoch 998, loss 0.0313, train loss 0.500, time 1.8 sec, n=252\n",
      "epoch 999, loss 0.0313, train loss 0.500, time 1.8 sec, n=252\n",
      "epoch 1000, loss 0.0313, train loss 0.500, time 1.8 sec, n=252\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss(reduction = 'sum')\n",
    "train(net, train_data, criterion,100, device, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (700) : an illegal memory access was encountered at ..\\torch/csrc/generic/serialization.cpp:31",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-a2590410039e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./data/HM_learn_scores_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ss20181116001\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ss20181116001\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[1;31m# Copy to a buffer, then serialize that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m             \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m             \u001b[0mbuf_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m             \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (700) : an illegal memory access was encountered at ..\\torch/csrc/generic/serialization.cpp:31"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), './data/HM_learn_scores_' + str(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\caffe2\\serialize\\inline_container.cc:209] . file not found: archive/data/1738475360880",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-06753189add9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/HM_learn_scores_2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ss20181116001\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    582\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ss20181116001\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ss20181116001\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    832\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m             \u001b[0mload_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ss20181116001\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[1;34m(data_type, size, key, location)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m         \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\caffe2\\serialize\\inline_container.cc:209] . file not found: archive/data/1738475360880"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('./data/HM_learn_scores_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 5  | 0 -1 0 1 2 -2 1 -1 -2 1 0 3 \n",
      "0 0 0 0 1 4  | 0 0 0 0 -1 0 -1 0 1 0 0 -1 \n",
      "0 0 0 0 2 3  | 0 0 0 0 -1 0 0 0 -1 0 0 -1 \n",
      "0 0 0 0 3 2  | 0 0 0 0 -1 0 0 0 0 0 0 -1 \n",
      "0 0 0 0 4 1  | 0 -1 -1 0 1 0 0 1 1 -1 0 0 \n",
      "0 0 0 0 5 0  | 0 -1 0 -1 1 0 1 0 0 0 0 0 \n",
      "0 0 0 1 0 4  | 0 0 0 0 -2 1 0 1 2 -1 0 -2 \n",
      "0 0 0 1 1 3  | 0 0 0 -1 1 0 0 0 -1 0 0 0 \n",
      "0 0 0 1 2 2  | 0 0 0 0 0 0 0 0 -1 0 0 0 \n",
      "0 0 0 1 3 1  | 0 0 0 0 1 0 1 0 0 0 0 0 \n",
      "0 0 0 1 4 0  | 0 0 0 1 0 -1 1 0 1 0 0 0 \n",
      "0 0 0 2 0 3  | 0 0 0 0 -1 0 -1 1 2 0 0 -2 \n",
      "0 0 0 2 1 2  | 0 1 0 -1 0 0 0 0 -1 0 0 1 \n",
      "0 0 0 2 2 1  | -1 1 0 -1 0 0 1 0 -1 0 0 -1 \n",
      "0 0 0 2 3 0  | 1 0 -1 0 1 1 0 1 1 0 1 -1 \n",
      "0 0 0 3 0 2  | 0 0 0 0 -1 0 -1 1 1 0 0 -2 \n",
      "0 0 0 3 1 1  | -1 1 0 -1 0 0 0 0 -1 0 0 1 \n",
      "0 0 0 3 2 0  | 1 0 -1 0 1 -1 -1 1 1 0 0 -1 \n",
      "0 0 0 4 0 1  | 1 0 0 -1 0 0 0 1 0 0 0 0 \n",
      "0 0 0 4 1 0  | -1 0 0 1 0 -1 0 1 1 0 0 -1 \n",
      "0 0 0 5 0 0  | -1 0 0 -1 -1 1 0 -1 0 0 0 0 \n",
      "0 0 1 0 0 4  | 0 0 0 -1 -2 1 -1 1 1 0 0 -1 \n",
      "0 0 1 0 1 3  | 0 0 0 0 1 -1 0 -1 0 -1 0 1 \n",
      "0 0 1 0 2 2  | 0 0 1 1 0 -1 0 1 0 0 0 -1 \n",
      "0 0 1 0 3 1  | 0 -1 1 1 1 -1 1 -1 0 -1 0 0 \n",
      "0 0 1 0 4 0  | 0 0 0 0 0 0 0 -1 0 1 0 0 \n",
      "0 0 1 1 0 3  | 0 0 0 0 0 0 0 -2 -3 -1 0 3 \n",
      "0 0 1 1 1 2  | 0 0 0 0 0 -1 0 0 0 0 0 0 \n",
      "0 0 1 1 2 1  | 0 0 1 0 0 0 0 0 0 1 0 0 \n",
      "0 0 1 1 3 0  | 0 0 0 0 0 1 1 1 0 0 0 -1 \n",
      "0 0 1 2 0 2  | 0 0 0 -1 0 0 0 0 0 -1 0 0 \n",
      "0 0 1 2 1 1  | 0 1 0 -1 0 0 0 1 0 2 0 -1 \n",
      "0 0 1 2 2 0  | 0 1 0 -1 0 1 -1 0 0 -1 0 0 \n",
      "0 0 1 3 0 1  | 0 1 0 -1 1 0 1 1 0 0 0 -1 \n",
      "0 0 1 3 1 0  | -1 1 0 0 0 -1 0 -1 0 0 0 0 \n",
      "0 0 1 4 0 0  | 0 0 1 0 -1 -1 0 2 1 -1 0 0 \n",
      "0 0 2 0 0 3  | 0 0 0 0 -1 0 -1 1 1 0 0 -1 \n",
      "0 0 2 0 1 2  | 0 -1 1 0 1 -1 0 0 0 -1 0 1 \n",
      "0 0 2 0 2 1  | 0 0 1 0 1 -1 0 1 -1 0 0 0 \n",
      "0 0 2 0 3 0  | 1 0 0 0 0 1 -1 0 1 1 0 0 \n",
      "0 0 2 1 0 2  | 0 0 0 0 1 0 0 0 -1 0 0 0 \n",
      "0 0 2 1 1 1  | 0 0 0 0 0 -1 0 0 0 1 0 0 \n",
      "0 0 2 1 2 0  | 0 0 0 0 0 0 -1 -1 0 -1 0 0 \n",
      "0 0 2 2 0 1  | 0 0 0 0 1 0 0 0 -1 0 0 0 \n",
      "0 0 2 2 1 0  | 0 0 0 -1 0 -1 0 0 -1 0 0 0 \n",
      "0 0 2 3 0 0  | 0 0 0 0 -1 -1 0 2 1 0 0 -1 \n",
      "0 0 3 0 0 2  | 0 1 0 0 -1 0 -1 0 1 -1 0 1 \n",
      "0 0 3 0 1 1  | 0 0 0 0 1 -1 0 -1 0 1 0 2 \n",
      "0 0 3 0 2 0  | 0 0 0 0 0 0 -1 0 0 1 0 0 \n",
      "0 0 3 1 0 1  | 0 0 0 -1 1 0 1 -1 0 0 0 1 \n",
      "0 0 3 1 1 0  | 0 0 0 -1 0 -1 0 -2 0 0 0 2 \n",
      "0 0 3 2 0 0  | 0 0 0 0 -1 -1 -1 1 1 -1 0 1 \n",
      "0 0 4 0 0 1  | 0 2 -1 -1 -1 1 0 1 0 -1 0 -4 \n",
      "0 0 4 0 1 0  | -1 1 0 0 0 -1 0 1 0 -1 0 -3 \n",
      "0 0 4 1 0 0  | 0 0 0 0 0 0 0 3 1 1 0 -3 \n",
      "0 0 5 0 0 0  | 0 -1 0 1 0 0 0 -1 -1 0 0 2 \n",
      "0 1 0 0 0 4  | 0 0 0 -1 0 0 0 0 0 0 0 -2 \n",
      "0 1 0 0 1 3  | 0 -1 0 0 1 0 0 -1 -2 1 0 0 \n",
      "0 1 0 0 2 2  | 0 0 0 0 0 0 1 0 0 1 0 0 \n",
      "0 1 0 0 3 1  | 0 0 0 0 1 0 1 0 0 0 0 0 \n",
      "0 1 0 0 4 0  | 0 1 0 0 0 1 0 0 -1 0 0 0 \n",
      "0 1 0 1 0 3  | 0 0 0 -1 0 0 0 0 -1 1 0 0 \n",
      "0 1 0 1 1 2  | 0 0 0 -1 0 0 0 0 -1 0 0 0 \n",
      "0 1 0 1 2 1  | 0 0 0 0 0 0 0 -1 0 0 0 0 \n",
      "0 1 0 1 3 0  | 0 0 0 0 0 0 1 0 -2 1 0 0 \n",
      "0 1 0 2 0 2  | 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "0 1 0 2 1 1  | 0 1 0 -1 0 0 0 1 1 0 0 -1 \n",
      "0 1 0 2 2 0  | 1 0 -1 0 0 1 -1 0 1 1 0 -1 \n",
      "0 1 0 3 0 1  | 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "0 1 0 3 1 0  | 0 0 -1 1 0 -1 0 -1 0 0 0 0 \n",
      "0 1 0 4 0 0  | 1 0 0 1 0 1 1 0 0 0 0 0 \n",
      "0 1 1 0 0 3  | 0 0 0 0 0 0 0 -1 -1 2 0 0 \n",
      "0 1 1 0 1 2  | 0 0 0 0 0 0 0 0 1 -1 0 0 \n",
      "0 1 1 0 2 1  | 0 0 0 0 -1 1 0 1 -1 0 0 -1 \n",
      "0 1 1 0 3 0  | 0 0 0 -1 0 1 0 0 -1 0 0 0 \n",
      "0 1 1 1 0 2  | 0 0 0 0 0 0 0 0 1 1 0 -1 \n",
      "0 1 1 1 1 1  | 0 0 0 0 0 0 0 -1 0 1 0 1 \n",
      "0 1 1 1 2 0  | 0 0 0 0 0 0 0 0 2 0 0 0 \n",
      "0 1 1 2 0 1  | 0 0 0 1 0 0 0 1 0 2 0 -2 \n",
      "0 1 1 3 0 0  | -1 0 1 -1 2 0 2 0 -1 0 0 0 \n",
      "0 1 2 0 0 2  | 0 0 1 1 0 0 0 1 0 0 0 0 \n",
      "0 1 2 0 1 1  | 0 0 1 1 -1 0 0 1 1 0 0 -1 \n",
      "0 1 2 0 2 0  | 0 0 1 0 0 0 0 0 -1 0 0 0 \n",
      "0 1 2 1 0 1  | 0 -1 0 1 0 0 0 1 2 0 0 -2 \n",
      "0 1 2 1 1 0  | 0 0 0 0 0 0 0 1 1 1 0 -1 \n",
      "0 1 2 2 0 0  | 0 0 0 0 0 0 0 -1 -1 1 0 1 \n",
      "0 1 3 0 0 1  | 0 0 1 1 -1 0 1 -1 -1 0 0 3 \n",
      "0 1 3 0 1 0  | 0 0 1 -1 0 0 0 -1 0 0 0 2 \n",
      "0 1 3 1 0 0  | 0 0 0 -1 1 1 1 0 -1 0 0 0 \n",
      "0 1 4 0 0 0  | 0 1 0 -1 1 0 0 0 0 0 0 -1 \n",
      "0 2 0 0 0 3  | 0 0 0 0 1 0 0 1 1 -2 0 0 \n",
      "0 2 0 0 1 2  | 0 0 0 0 1 0 0 -1 1 0 0 1 \n",
      "0 2 0 0 2 1  | 0 0 0 0 0 0 1 1 -1 1 0 0 \n",
      "0 2 0 0 3 0  | -1 0 0 0 0 0 1 0 1 -1 0 -1 \n",
      "0 2 0 1 0 2  | 0 0 0 0 0 0 0 -1 -1 0 0 2 \n",
      "0 2 0 1 1 1  | 0 1 0 -1 0 0 0 0 0 0 0 1 \n",
      "0 2 0 1 2 0  | 0 0 0 1 0 0 1 0 0 0 0 0 \n",
      "0 2 0 2 0 1  | 0 0 0 0 0 0 0 0 -1 0 0 1 \n",
      "0 2 0 2 1 0  | 0 0 0 0 0 0 0 0 0 0 0 -1 \n",
      "0 2 0 3 0 0  | 0 0 0 1 0 1 0 0 0 0 0 0 \n",
      "0 2 1 0 1 1  | 0 0 0 0 0 0 0 1 1 -1 0 -1 \n",
      "0 2 1 0 2 0  | 0 0 0 -1 0 0 0 0 -2 0 0 0 \n",
      "0 2 1 1 0 1  | 0 0 0 0 0 0 0 0 2 0 0 -1 \n",
      "0 2 1 1 1 0  | 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "0 2 1 2 0 0  | 0 0 0 0 1 0 1 -1 0 -1 0 1 \n",
      "0 2 2 0 0 1  | 0 0 0 1 -1 0 0 0 1 0 0 -1 \n",
      "0 2 2 0 1 0  | 0 1 -1 0 -1 0 -1 0 -1 1 0 0 \n",
      "0 2 2 1 0 0  | 0 0 -1 0 0 0 0 0 0 0 0 -1 \n",
      "0 2 3 0 0 0  | 0 0 0 0 1 -1 1 0 1 0 0 1 \n",
      "0 3 0 0 0 2  | 0 0 0 0 0 0 0 0 1 0 0 0 \n",
      "0 3 0 0 1 1  | 0 0 0 0 1 0 1 -1 0 0 0 2 \n",
      "0 3 0 0 2 0  | 0 1 0 0 0 0 0 0 1 -1 0 -1 \n",
      "0 3 0 1 0 1  | 0 0 0 0 0 0 0 -1 -1 -1 0 2 \n",
      "0 3 0 1 1 0  | 0 0 0 0 0 0 0 0 -1 1 0 0 \n",
      "0 3 0 2 0 0  | 0 0 0 0 0 1 1 0 0 0 0 1 \n",
      "0 3 1 0 1 0  | 0 1 -1 -1 1 0 0 -1 -1 1 0 2 \n",
      "0 3 1 1 0 0  | 0 0 0 -1 1 0 1 -1 -1 -2 0 2 \n",
      "0 3 2 0 0 0  | 0 1 -1 -1 1 -1 0 -1 0 1 0 1 \n",
      "0 4 0 0 0 1  | 0 0 0 0 0 0 0 1 0 1 0 0 \n",
      "0 4 0 0 1 0  | 0 1 0 0 0 1 0 1 1 2 0 0 \n",
      "0 4 0 1 0 0  | 0 0 0 0 -1 1 1 1 1 1 0 -1 \n",
      "0 4 1 0 0 0  | 0 0 0 -1 0 0 0 0 0 0 0 0 \n",
      "0 5 0 0 0 0  | 0 -1 0 1 0 -1 0 -2 -1 -1 0 1 \n",
      "1 0 0 0 0 4  | 0 1 0 0 -1 1 1 0 0 0 0 0 \n",
      "1 0 0 0 1 3  | 0 0 0 0 1 0 0 0 0 0 0 0 \n",
      "1 0 0 0 2 2  | 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "1 0 0 0 3 1  | 0 0 0 0 0 1 1 0 0 -1 0 1 \n",
      "1 0 0 0 4 0  | 0 0 -1 0 -1 0 -1 0 0 0 0 0 \n",
      "1 0 0 1 0 3  | 0 -1 0 0 -1 0 0 -1 0 1 0 1 \n",
      "1 0 0 1 1 2  | 0 0 0 1 -1 0 -1 0 1 0 0 0 \n",
      "1 0 0 1 2 1  | 0 0 0 1 -1 0 0 -1 0 0 0 2 \n",
      "1 0 0 1 3 0  | 0 -1 0 1 1 0 1 0 0 -1 0 1 \n",
      "1 0 0 2 0 2  | 0 -1 0 1 -1 0 0 -1 0 1 0 2 \n",
      "1 0 0 2 1 1  | 0 0 0 0 0 0 0 0 1 0 0 0 \n",
      "1 0 0 2 2 0  | 0 0 0 0 0 0 1 0 -1 -1 0 2 \n",
      "1 0 0 3 0 1  | 0 -1 0 1 0 0 0 -1 -1 1 0 3 \n",
      "1 0 0 3 1 0  | 0 -1 0 0 1 0 0 -2 -1 0 0 1 \n",
      "1 0 0 4 0 0  | 1 0 0 1 -1 0 0 0 0 1 0 1 \n",
      "1 0 1 0 0 3  | 0 0 -1 0 0 1 0 -1 -2 1 0 2 \n",
      "1 0 1 0 1 2  | 0 0 0 0 0 0 0 0 0 -1 0 0 \n",
      "1 0 1 0 2 1  | 0 0 0 0 0 0 0 0 2 -1 0 0 \n",
      "1 0 1 0 3 0  | 0 0 0 0 0 0 0 0 0 -1 0 1 \n",
      "1 0 1 1 0 2  | 0 0 0 0 0 1 0 0 1 -1 0 0 \n",
      "1 0 1 1 1 1  | 0 0 0 0 -2 1 0 0 0 -2 0 1 \n",
      "1 0 1 1 2 0  | 0 0 0 0 -1 1 0 0 0 0 0 0 \n",
      "1 0 1 2 0 1  | 0 0 -1 0 0 0 0 0 2 -1 0 -1 \n",
      "1 0 1 2 1 0  | 0 0 0 -1 0 1 0 0 3 0 0 0 \n",
      "1 0 1 3 0 0  | 0 0 0 1 -1 0 -1 -2 -1 -1 0 2 \n",
      "1 0 2 0 0 2  | 0 0 0 0 0 1 1 -1 -1 0 0 2 \n",
      "1 0 2 0 1 1  | 0 0 0 0 0 1 0 1 0 0 0 -1 \n",
      "1 0 2 1 0 1  | 0 0 0 0 0 1 1 -1 1 -1 0 1 \n",
      "1 0 2 1 1 0  | 0 0 0 0 -1 1 0 -1 1 0 0 1 \n",
      "1 0 2 2 0 0  | 0 -1 0 0 -1 0 -1 -1 -1 0 0 0 \n",
      "1 0 3 0 0 1  | 0 0 0 1 0 1 1 -1 -1 1 0 2 \n",
      "1 0 3 0 1 0  | 0 0 0 0 1 0 0 -1 1 0 0 0 \n",
      "1 0 3 1 0 0  | 0 -1 0 1 0 0 0 -1 -1 -1 0 2 \n",
      "1 0 4 0 0 0  | 0 0 0 1 -1 0 0 1 1 0 0 0 \n",
      "1 1 0 0 0 3  | 0 0 0 -1 1 0 0 0 -1 0 0 0 \n",
      "1 1 0 0 1 2  | 0 0 0 0 0 0 0 0 1 1 0 0 \n",
      "1 1 0 0 2 1  | 0 0 0 0 -1 0 0 0 1 0 0 0 \n",
      "1 1 0 0 3 0  | 0 0 0 0 0 0 0 -1 0 0 0 1 \n",
      "1 1 0 1 0 2  | 0 0 0 0 0 0 0 1 2 0 0 -1 \n",
      "1 1 0 1 1 1  | 0 0 0 1 -1 -1 -1 1 1 -1 0 -1 \n",
      "1 1 0 1 2 0  | 0 0 0 1 -1 0 0 0 1 -1 0 0 \n",
      "1 1 0 2 0 1  | 0 -1 0 1 0 0 0 1 1 0 0 0 \n",
      "1 1 0 2 1 0  | 0 0 0 0 0 0 0 0 2 0 0 -1 \n",
      "1 1 0 3 0 0  | 0 -1 0 1 0 0 0 0 0 0 0 0 \n",
      "1 1 1 0 0 2  | 0 0 -1 0 -1 1 -1 0 1 0 0 0 \n",
      "1 1 1 0 1 1  | 0 0 0 0 -1 1 0 1 -1 0 0 -1 \n",
      "1 1 1 0 2 0  | 0 0 0 0 -1 0 -1 0 0 0 0 0 \n",
      "1 1 1 1 0 1  | 0 0 0 0 0 0 0 0 -1 2 0 0 \n",
      "1 1 1 1 1 0  | 0 0 0 0 -1 1 0 0 -1 0 0 0 \n",
      "1 1 1 2 0 0  | 0 -1 0 0 0 1 0 1 -1 0 0 0 \n",
      "1 1 2 0 0 1  | 0 0 0 1 -1 1 1 0 1 -1 0 0 \n",
      "1 1 2 0 1 0  | 0 0 0 0 -1 0 -1 0 1 -1 0 0 \n",
      "1 1 2 1 0 0  | 0 -1 0 0 0 1 0 1 0 0 0 -1 \n",
      "1 1 3 0 0 0  | 0 0 1 0 0 0 0 0 -1 0 0 1 \n",
      "1 2 0 0 0 2  | 0 0 0 0 0 0 0 1 -1 1 0 0 \n",
      "1 2 0 0 1 1  | 0 0 0 0 0 0 0 1 0 0 0 -1 \n",
      "1 2 0 0 2 0  | 0 0 0 0 0 -1 -1 1 -1 -1 0 -1 \n",
      "1 2 0 1 0 1  | 0 0 0 0 0 0 0 1 1 0 0 -1 \n",
      "1 2 0 1 1 0  | 0 0 0 0 0 -1 -1 1 1 -1 0 -1 \n",
      "1 2 0 2 0 0  | 0 0 0 0 0 0 0 0 -1 0 0 -1 \n",
      "1 2 1 0 0 1  | 0 0 0 0 -1 0 -1 1 1 -1 0 -1 \n",
      "1 2 1 0 1 0  | 0 0 -1 0 -1 0 -2 1 -1 1 0 -1 \n",
      "1 2 1 1 0 0  | 0 0 0 0 0 0 0 0 -1 0 0 0 \n",
      "1 2 2 0 0 0  | 0 0 0 0 -1 -1 -1 0 -1 0 0 -1 \n",
      "1 3 0 0 0 1  | 0 0 0 0 0 0 0 1 -2 1 0 0 \n",
      "1 3 0 0 1 0  | 0 0 0 0 0 0 0 0 0 1 0 0 \n",
      "1 3 0 1 0 0  | 0 0 0 0 0 0 0 0 1 0 0 0 \n",
      "1 3 1 0 0 0  | 0 0 0 0 0 -1 -1 0 -1 -1 0 0 \n",
      "1 4 0 0 0 0  | 0 0 0 0 0 0 0 2 1 0 0 -1 \n",
      "2 0 0 0 0 3  | 0 1 -1 0 0 1 1 -1 2 -1 0 0 \n",
      "2 0 0 0 1 2  | 0 0 0 0 0 -1 0 0 0 0 0 -1 \n",
      "2 0 0 0 2 1  | 0 0 0 0 0 0 0 0 -1 1 0 0 \n",
      "2 0 0 0 3 0  | 0 0 -1 0 0 -1 0 1 0 1 0 -1 \n",
      "2 0 0 1 0 2  | 0 0 0 1 0 -1 -1 1 0 -1 0 -1 \n",
      "2 0 0 1 1 1  | 0 0 0 1 -1 -1 -1 1 0 -1 0 -1 \n",
      "2 0 0 1 2 0  | 0 -1 0 1 0 0 0 0 -1 0 0 0 \n",
      "2 0 0 2 0 1  | 0 -1 0 1 0 -1 0 0 -1 0 0 0 \n",
      "2 0 0 2 1 0  | 0 -1 0 0 1 0 0 -1 -1 0 0 1 \n",
      "2 0 0 3 0 0  | 0 -1 0 0 0 1 0 1 0 0 0 0 \n",
      "2 0 1 0 0 2  | 0 0 0 0 1 0 1 0 -1 1 0 0 \n",
      "2 0 1 0 1 1  | 0 0 0 0 1 0 0 1 1 0 0 -1 \n",
      "2 0 1 0 2 0  | 0 0 0 0 0 0 0 1 -1 0 0 -1 \n",
      "2 0 1 1 0 1  | 0 0 0 1 0 0 0 1 0 -1 0 -1 \n",
      "2 0 1 1 1 0  | 0 0 0 0 0 0 0 1 0 1 0 -2 \n",
      "2 0 1 2 0 0  | 0 -1 0 0 0 0 0 0 -2 0 0 -1 \n",
      "2 0 2 0 0 1  | 0 0 0 0 1 0 1 0 -1 1 0 0 \n",
      "2 0 2 0 1 0  | 0 0 0 0 1 0 0 1 0 0 0 -1 \n",
      "2 0 2 1 0 0  | 0 -1 0 1 0 0 0 1 -2 2 0 -2 \n",
      "2 0 3 0 0 0  | 0 0 1 1 0 -1 1 0 1 0 0 0 \n",
      "2 1 0 0 0 2  | 0 0 0 -1 0 0 0 1 -1 0 0 0 \n",
      "2 1 0 0 1 1  | 0 0 0 0 0 0 0 1 0 -1 0 -1 \n",
      "2 1 0 0 2 0  | 0 0 0 0 0 0 0 0 1 1 0 0 \n",
      "2 1 0 1 0 1  | 0 0 0 1 0 -1 0 0 0 -1 0 -1 \n",
      "2 1 0 2 0 0  | 0 -1 0 1 0 0 0 0 -1 0 0 -1 \n",
      "2 1 1 0 0 1  | 0 0 0 0 0 0 0 1 2 0 0 -1 \n",
      "2 1 1 0 1 0  | 0 0 -1 0 0 0 -1 0 2 -1 0 -1 \n",
      "2 1 1 1 0 0  | 0 -1 0 0 0 0 0 0 0 0 0 0 \n",
      "2 1 2 0 0 0  | 0 0 1 0 0 0 0 1 0 0 0 -1 \n",
      "2 2 0 0 0 1  | 0 0 0 0 0 0 0 0 -1 0 0 1 \n",
      "2 2 0 0 1 0  | 0 0 0 -1 0 0 0 0 0 1 0 0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 0 1 0 0  | 0 0 0 0 0 0 0 0 0 -1 0 0 \n",
      "2 2 1 0 0 0  | 0 0 0 0 0 0 -1 0 0 0 0 0 \n",
      "2 3 0 0 0 0  | 0 1 0 0 0 0 0 0 0 0 0 0 \n",
      "3 0 0 0 0 2  | 0 1 0 0 0 0 0 -1 0 0 0 0 \n",
      "3 0 0 0 1 1  | 0 0 0 0 0 0 0 -1 -1 1 0 1 \n",
      "3 0 0 0 2 0  | 0 0 0 0 0 0 0 -2 0 0 0 1 \n",
      "3 0 0 1 0 1  | 0 -1 0 1 0 -1 0 -1 1 0 0 0 \n",
      "3 0 0 1 1 0  | 0 -1 0 0 0 0 0 -1 -1 0 0 1 \n",
      "3 0 0 2 0 0  | 0 -1 0 0 0 0 0 0 2 0 0 0 \n",
      "3 0 1 0 0 1  | 0 0 0 0 1 0 1 -1 1 1 0 1 \n",
      "3 0 1 0 1 0  | 0 0 0 0 0 0 0 -1 0 0 0 1 \n",
      "3 0 1 1 0 0  | 0 -1 1 1 0 0 1 -1 0 0 0 0 \n",
      "3 0 2 0 0 0  | 0 0 1 0 0 0 0 -1 2 0 0 1 \n",
      "3 1 0 0 0 1  | 0 1 0 -1 0 0 0 -2 0 0 0 3 \n",
      "3 1 0 0 1 0  | 0 0 0 -1 0 0 0 -3 0 0 0 3 \n",
      "3 1 0 1 0 0  | 0 0 0 0 0 0 0 -2 1 0 0 1 \n",
      "3 1 1 0 0 0  | 0 0 0 0 0 1 0 -2 1 0 0 2 \n",
      "3 2 0 0 0 0  | 0 1 0 -1 0 0 0 -1 2 0 0 0 \n",
      "4 0 0 0 0 1  | 0 1 0 -1 0 0 0 1 -2 0 0 -2 \n",
      "4 0 0 0 1 0  | 0 0 0 -1 0 0 0 2 0 -1 0 -1 \n",
      "4 0 0 1 0 0  | 0 -1 0 0 0 0 0 3 1 1 0 -1 \n",
      "4 0 1 0 0 0  | 0 0 1 0 0 0 0 2 -1 -1 0 -1 \n",
      "4 1 0 0 0 0  | 0 1 -1 -1 0 1 -1 0 -1 0 0 -1 \n",
      "5 0 0 0 0 0  | 0 1 0 -1 0 0 0 1 0 0 0 1 \n",
      "5\n"
     ]
    }
   ],
   "source": [
    "corr = 0\n",
    "\n",
    "for x,y in train_data:\n",
    "    net.eval()\n",
    "    x,y = x.to(device),y.to(device)\n",
    "    y_hat = net(x)\n",
    "\n",
    "    hy = y.clone().detach()\n",
    "    hyh = y_hat.clone().detach()\n",
    "    \n",
    "    dh = (hy-hyh)\n",
    "    \n",
    "    flag = False\n",
    "\n",
    "    \n",
    "    for i in range(OUTPUT_SIZE):\n",
    "        if not unpack(dh[i]) == 0:\n",
    "            flag = True\n",
    "            break\n",
    "    \n",
    "    if not flag:\n",
    "        corr += 1\n",
    "        continue\n",
    "    \n",
    "    cont = \"\"\n",
    "    for a in x:\n",
    "        cont += str(int(a*5)) + \" \"\n",
    "    cont += \" | \"\n",
    "    for d in hy-hyh:\n",
    "    #for d in hyh:\n",
    "        cont += str(unpack(d)) + \" \"\n",
    "    print(cont)\n",
    "        \n",
    "print(\"{0}\".format(corr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-5e54212c5169>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "net(torch.tensor([0,0,0,0,0,5], dtype = torch.float32, device = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saveidx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9c72818a71bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msaveidx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'saveidx' is not defined"
     ]
    }
   ],
   "source": [
    "saveidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
