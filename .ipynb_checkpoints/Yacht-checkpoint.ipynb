{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "from collections import Counter\n",
    "sys.path.insert(0, '..')\n",
    "#import d2l\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random as rand\n",
    "\n",
    "def count_func(num):\n",
    "    return lambda dice: num * Counter(dice)[num]\n",
    "\n",
    "def four_of_a_kind(dice):\n",
    "    for k, v in Counter(dice).items():\n",
    "        if 4 <= v:\n",
    "            return k * 4\n",
    "    return 0\n",
    "\n",
    "def straight(dice):\n",
    "    min_dice = min(dice)\n",
    "    length = 1\n",
    "    while min_dice + length in dice:\n",
    "        length += 1\n",
    "    return length\n",
    "\n",
    "YACHT = lambda dice: 50 if len(dice) == 5 and len(set(dice)) == 1 else 0\n",
    "ONES = count_func(1)\n",
    "TWOS = count_func(2)\n",
    "THREES = count_func(3)\n",
    "FOURS = count_func(4)\n",
    "FIVES = count_func(5)\n",
    "SIXES = count_func(6)\n",
    "FULL_HOUSE = lambda dice: sum(dice) if \\\n",
    "    sorted(tuple(Counter(dice).values())) == [2, 3] else 0\n",
    "FOUR_OF_A_KIND = four_of_a_kind\n",
    "SMALL_STRAIGHT = lambda dice: 15 if straight(dice) >= 4 else 0\n",
    "LARGE_STRAIGHT = lambda dice: 30 if 5 == straight(dice) else 0\n",
    "CHOICE = lambda dice: sum(dice)\n",
    "def score(dice, category):\n",
    "    return category(dice)\n",
    "\n",
    "class yacht_game:\n",
    "    score_board = torch.tensor([[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "                                [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]], requires_grad = False)\n",
    "    dice_status = torch.zeros(5, requires_grad = False)\n",
    "    roll_count = torch.tensor([2],requires_grad = False)\n",
    "    total_score = [0, 0]\n",
    "    cur_player = 0\n",
    "    multi_mode = False\n",
    "    score_func = [ONES, TWOS, THREES, FOURS, FIVES,\n",
    "              SIXES, CHOICE, FOUR_OF_A_KIND, FULL_HOUSE,\n",
    "              SMALL_STRAIGHT, LARGE_STRAIGHT,YACHT]\n",
    "            \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset_game()\n",
    "\n",
    "    def roll_dice(self, roll_action_num):\n",
    "        for i in range(5):\n",
    "            if roll_action_num%2 == 1:\n",
    "                self.dice_status[i] = rand.randint(1, 6)\n",
    "            roll_action_num /= 2\n",
    "        self.roll_count -= 1\n",
    "\n",
    "    def set_multi_mode(self,mode):\n",
    "        self.multi_mode = mode\n",
    "    \n",
    "    def get_yacht_output(self):\n",
    "        cur_total_score = 0\n",
    "        for i in range(len(self.score_board[self.cur_player])):\n",
    "            cur_total_score += self.score_board[self.cur_player][i]\n",
    "            if self.score_board[self.cur_player][i] == -1:\n",
    "                cur_total_score += 1\n",
    "        if self.multi_mode:\n",
    "            return torch.cat((self.score_board[self.cur_player], self.score_board[1 - self.cur_player], self.roll_count, self.dice_status)), cur_total_score, self.is_game_finished()\n",
    "        else:\n",
    "            return torch.cat((self.score_board[self.cur_player], self.roll_count, self.dice_status))\\\n",
    "                    ,cur_total_score, self.is_game_finished()\n",
    "\n",
    "    def is_game_finished(self):\n",
    "        return -1 not in self.score_board[self.cur_player]\n",
    "\n",
    "    def set_score(self, dice, category):\n",
    "        self.score_board[self.cur_player][category] = score(dice, self.score_func[category])\n",
    "    \n",
    "    def update(self, yacht_input):\n",
    "        if self.is_game_finished():\n",
    "            #print('Game End')\n",
    "            #print('player : ', self.cur_player, ' total score : ', self.total_score[self.cur_player])\n",
    "            if self.multi_mode:\n",
    "                self.cur_player = 1 - self.cur_player\n",
    "        else:\n",
    "            dice_input, score_input = yacht_input[:32], yacht_input[32:]\n",
    "            \n",
    "            max_dice_index = dice_input.index(max(dice_input))\n",
    "            if self.roll_count > 0 and not max_dice_index == 0:\n",
    "                #print('Roll dice')\n",
    "                self.roll_dice(max_dice_index)\n",
    "\n",
    "            else:  # Set score\n",
    "                #print('player : ', self.cur_player, ' Set score')\n",
    "                for pref, i in sorted(zip(score_input,range(len(score_input)) ), reverse=True):\n",
    "                    if self.score_board[self.cur_player][i] == -1:\n",
    "                        self.set_score(self.dice_status, i)\n",
    "                        break\n",
    "                    else:\n",
    "                        score_input[i] = -math.inf\n",
    "                self.roll_count = 3\n",
    "                self.roll_dice(31)\n",
    "\n",
    "                if self.is_game_finished():  # Game Ended\n",
    "                    #print('Game End')\n",
    "                    bonus_counter, score_sum = 0, 0\n",
    "                    for i in self.score_board[self.cur_player]:\n",
    "                        score_sum += i\n",
    "                        if i < 6:\n",
    "                            bonus_counter += i\n",
    "                    if bonus_counter >= 63:\n",
    "                        score_sum += 35\n",
    "                    self.total_score[self.cur_player] = score_sum\n",
    "                    #print('player : ', self.cur_player, ' total score : ', score_sum)\n",
    "\n",
    "                if self.multi_mode:\n",
    "                    self.cur_player = 1 - self.cur_player\n",
    "    \n",
    "    def reset_game(self):\n",
    "        score_board = torch.tensor([[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "                                    [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]], requires_grad = False)\n",
    "        dice_status = torch.zeros(5, requires_grad = False)\n",
    "        roll_count = torch.tensor([2],requires_grad = False)\n",
    "        total_score = [0, 0]\n",
    "        cur_player = 0\n",
    "\n",
    "game = yacht_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "game.get_yacht_output() \\\n",
    "game.update(yacht_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (fc1): Linear(in_features=18, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (fc6): Linear(in_features=50, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_SIZE = 18\n",
    "OUTPUT_SIZE = 44\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        #self.fc3 = nn.Linear(100, 100)\n",
    "        #self.fc4 = nn.Linear(100, 100)\n",
    "        #self.fc5 = nn.Linear(100, 100)\n",
    "        self.fc6 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        #x = F.relu(self.fc4(x))\n",
    "        #x = F.relu(self.fc5(x))\n",
    "\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "policy_net = DQN(INPUT_SIZE,OUTPUT_SIZE).to(device)\n",
    "target_net = DQN(INPUT_SIZE,OUTPUT_SIZE).to(device)\n",
    "\n",
    "\n",
    "policy_net.apply(init_weights)\n",
    "target_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.099\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 500\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(200)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
    "            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
    "            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
    "            return policy_net(state)\n",
    "    else:\n",
    "        return torch.randn(OUTPUT_SIZE, device=device)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    #if is_ipython:\n",
    "    #    display.clear_output(wait=True)\n",
    "    #    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    #print(batch.next_state)\n",
    "\n",
    "    # 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다\n",
    "    # (최종 상태는 시뮬레이션이 종료 된 이후의 상태)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat(batch.next_state)\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    #print(str(policy_net(state_batch).size()))\n",
    "\n",
    "    # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
    "    # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
    "    state_action_values = policy_net(state_batch)\n",
    "\n",
    "    # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
    "    # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
    "    # max(1)[0]으로 최고의 보상을 선택하십시오.\n",
    "    # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values = target_net(non_final_next_states)\n",
    "    # 기대 Q 값 계산\n",
    "    expected_state_action_values = (next_state_values * GAMMA) * reward_batch\n",
    "    \n",
    "    # Huber 손실 계산\n",
    "    \n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "    # 모델 최적화\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ss20181116001\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'yacht' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-77893ab3584a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myacht\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_yacht_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_reward\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yacht' is not defined"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "for i_episode in range(num_episodes):\n",
    "    # 환경과 상태 초기화\n",
    "    game.reset_game()\n",
    "    state, reward, _ = game.get_yacht_output()\n",
    "    state = torch.tensor(state, dtype=torch.float, device=device)\n",
    "    for t in count():\n",
    "        # 행동 선택과 수행\n",
    "        action = select_action(state)\n",
    "        game.update(action.tolist())\n",
    "        new_state, new_reward, done = game.get_yacht_output()\n",
    "        new_state = torch.tensor(new_state, dtype=torch.float, device=device)\n",
    "        step_reward = torch.tensor([new_reward - reward], device=device)\n",
    "\n",
    "\n",
    "        # 메모리에 변이 저장\n",
    "        #print(state.size())\n",
    "        memory.push(state.reshape(1,INPUT_SIZE), action.reshape(1,OUTPUT_SIZE), new_state.reshape(1,INPUT_SIZE), torch.tensor([new_reward],device=device).view(1,1))\n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = new_state\n",
    "        reward = new_reward\n",
    "\n",
    "        # 최적화 한단계 수행(목표 네트워크에서)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            state, score, _ = yacht.get_yacht_output()\n",
    "            print(str(i_episode) + ') ' + str(state[:12]) + ' score : ' + str(score))\n",
    "            #plot_durations()\n",
    "            break\n",
    "    #목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
