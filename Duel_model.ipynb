{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import IPython as ip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import yacht_main as yacht\n",
    "from yacht_test import create_train_set\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(torch.nn.Module):\n",
    "    \"\"\"def __init__(self, size):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.input_size = size\"\"\"\n",
    "    def forward(self, x, *args):\n",
    "        return x.view(args)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.isize = input_size\n",
    "        self.diceonly = nn.Linear(input_size, output_size)\n",
    "        self.dice1 = nn.Linear(input_size, 300)\n",
    "        self.dice2 = nn.Linear(300, 300)\n",
    "        self.dice3 = nn.Linear(300, 50)\n",
    "        \n",
    "        self.diceconv = nn.Conv2d(1, 300, (1, input_size))\n",
    "        self.convshape = Reshape()\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "        #self.dice4 = nn.Linear(100, 100)\n",
    "        #self.dice5 = nn.Linear(100, 100)\n",
    "        #self.dice6 = nn.Linear(100, 100)\n",
    "        self.dice7 = nn.Linear(50, output_size)\n",
    "        #self.score1 = nn.Linear(12, 12)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.convshape(x, -1, 1, 1, self.isize)\n",
    "        #x = self.diceconv(x)\n",
    "        #x = self.convshape(x, -1, 300)\n",
    "        x = self.activation(self.dice1(x))\n",
    "        x = self.activation(self.dice2(x))\n",
    "        x = self.activation(self.dice3(x))\n",
    "        #x = torch.sigmoid(self.dice4(x))\n",
    "        #x = torch.sigmoid(self.dice5(x))\n",
    "        #x = torch.sigmoid(self.dice6(x))\n",
    "        x = self.dice7(x)\n",
    "        #x = self.diceonly(x)\n",
    "        return x\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.constant_(m,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.9\n",
    "EPS_END = 0.5\n",
    "EPS_DECAY = 10000000\n",
    "steps_done = 0\n",
    "INPUT_SIZE = 1\n",
    "OUTPUT_SIZE = 1\n",
    "TARGET_UPDATE = 20\n",
    "TRAINSET_UPDATE = 1000\n",
    "policy_net = None\n",
    "target_net = None\n",
    "\n",
    "def select_action(state, avail):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \"\"\"\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            policy_net(state)\n",
    "            return (policy_net(state) * avail).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(OUTPUT_SIZE)]], device=device, dtype=torch.long)\n",
    "    \"\"\"\n",
    "    if sample <= eps_threshold:\n",
    "        random_action = 0\n",
    "        randomizer = randint(1, avail.sum())\n",
    "        avail_list = torch.reshape(avail,[-1]).tolist()\n",
    "        for i in range(len(avail_list)):\n",
    "            if avail_list[i] == 1:\n",
    "                if randomizer == 1:\n",
    "                    random_action = i\n",
    "                    break\n",
    "                randomizer -= 1\n",
    "        return torch.tensor([[random_action]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            a = policy_net(state)\n",
    "            #a -= a.min()\n",
    "            return a.max(1)[1].view(1, 1)\n",
    "\n",
    "\n",
    "episode_scores = []\n",
    "episode_success = []\n",
    "episode_reward = []\n",
    "\n",
    "def plot_scores():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    scores_t = torch.tensor(episode_scores, dtype=torch.float)\n",
    "    success_t = torch.tensor(episode_success, dtype=torch.float)\n",
    "    reward_t = torch.tensor(episode_reward, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.plot(scores_t.numpy())\n",
    "    if len(scores_t) >= 50:\n",
    "        means = scores_t.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(49), means))\n",
    "        plt.plot(means.numpy())\n",
    "    if len(success_t) >= 50:\n",
    "        sucm = success_t.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        sucm = torch.cat((torch.zeros(49), sucm))\n",
    "        plt.plot(sucm.numpy())\n",
    "    if len(reward_t) >= 50:\n",
    "        rewm = reward_t.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        rewm = torch.cat((torch.zeros(49), rewm))\n",
    "        plt.plot(rewm.numpy())\n",
    "\n",
    "    ip.display.clear_output(wait=True)\n",
    "    plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_net = True\n",
    "def init_net(isize, osize, msize, lr = 0.001, g = 0, reset = False):\n",
    "    global policy_net, target_net, optimizer, memory, EPS_START, EPS_END, EPS_DECAY, steps_done, episode_scores, episode_success, episode_reward\n",
    "    global TARGET_UPDATE, INPUT_SIZE, OUTPUT_SIZE\n",
    "    \n",
    "    if reset:\n",
    "        INPUT_SIZE = isize\n",
    "        OUTPUT_SIZE = osize\n",
    "\n",
    "        MEMORY_SIZE = msize\n",
    "\n",
    "        TARGET_UPDATE = 5\n",
    "\n",
    "        memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "        policy_net = DQN(INPUT_SIZE,OUTPUT_SIZE).to(device)\n",
    "        target_net = DQN(INPUT_SIZE,OUTPUT_SIZE).to(device)\n",
    "\n",
    "\n",
    "        EPS_START = 0.9\n",
    "        EPS_END = 0.05\n",
    "        EPS_DECAY = 1000\n",
    "        steps_done = 0\n",
    "        GAMMA = g\n",
    "\n",
    "        episode_scores = []\n",
    "        episode_success = []\n",
    "        episode_reward = []\n",
    "    \n",
    "        \n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "    #init_weights(policy_net)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0\n",
    "reward_rate = 0.1\n",
    "regul = 0\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1)) + torch.norm(policy_net(state_batch)) * regul\n",
    "    \"\"\"print('state ex: ' + str(state_batch[0]))\n",
    "    print('action:' + str(action_batch.view(-1)))\n",
    "    print('reward:' + str(reward_batch.view(-1)))\n",
    "    print('value:' + str(state_action_values.view(-1)))\n",
    "    print('expected:' + str(expected_state_action_values.unsqueeze(1).view(-1)))\"\"\"\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #for param in policy_net.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    \"\"\"state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    print('after:' + str(state_action_values.view(-1)))\n",
    "    print('\\n')\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trainset():\n",
    "    print(\"Creating train set...\")\n",
    "    train_set_size = 3000 // 5\n",
    "    train_set = create_train_set(train_set_size)\n",
    "    for state, action, new_state, step_reward in train_set:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float, device=device, requires_grad = False)\n",
    "        action_tensor = torch.tensor([[action - 31]], device=device, dtype=torch.long)\n",
    "        new_state_tensor = torch.tensor(new_state, dtype=torch.float, device=device, requires_grad = False)\n",
    "        step_reward_tensor = torch.tensor([step_reward], device=device, requires_grad = False)\n",
    "        memory.push(state_tensor.reshape(1,INPUT_SIZE), action_tensor, new_state_tensor.reshape(1,INPUT_SIZE), step_reward_tensor)\n",
    "    print(\"Created\", train_set_size * 5, \"train set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def net1_main(lr, g):\n",
    "    global episode_scores, episode_success,episode_reward, reset_net\n",
    "    init_net(20, 12, 10000, lr=lr, g=g,reset = reset_net)\n",
    "    reset_net = False\n",
    "\n",
    "    num_episodes = 1000000\n",
    "    made_prob = 0\n",
    "    \n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        reward_total = 0\n",
    "        yacht.reset_game()\n",
    "        state, score, _, avail = yacht.get_yacht_output()\n",
    "        state = torch.tensor(state, dtype=torch.float, device=device, requires_grad = False)\n",
    "        for t in count():\n",
    "            avail = avail[31:]\n",
    "            avail = torch.tensor(avail, dtype=torch.float, device=device, requires_grad = False)\n",
    "            action = select_action(state.reshape(1,INPUT_SIZE), avail) + 31\n",
    "            reward = yacht.update(action)\n",
    "            if reward != -1:\n",
    "                reward_total+=reward\n",
    "\n",
    "            new_state, _, done, avail = yacht.get_yacht_output()\n",
    "            step_reward = torch.tensor([reward], device=device, requires_grad = False)\n",
    "            \n",
    "            #print(str(action) + ' ' + str(reward) + ' ' + str(yacht.dice_status))\n",
    "            \n",
    "            if random.random() < made_prob:\n",
    "                yacht.handled_roll()\n",
    "\n",
    "            if not done:\n",
    "                new_state = torch.tensor(new_state, dtype=torch.float, device=device, requires_grad = False)\n",
    "                memory.push(state.reshape(1,INPUT_SIZE), action - 31, new_state.reshape(1,INPUT_SIZE), \\\n",
    "                        step_reward)\n",
    "            else:\n",
    "                new_state = None\n",
    "                memory.push(state.reshape(1,INPUT_SIZE), action - 31, None, \\\n",
    "                        step_reward)\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "            optimize_model()\n",
    "            if done:\n",
    "                state, score, _, _ = yacht.get_yacht_output()\n",
    "                episode_success.append(state[:12].count(-1) * 10)\n",
    "                episode_scores.append(score)\n",
    "                episode_reward.append(reward_total * 10)\n",
    "                \n",
    "                #print(\"{0}) {1}\\tscore : {2}, turns = {3}\".format(i_episode, state[:12], score, t+1))\n",
    "\n",
    "                if i_episode % 200 == 0:\n",
    "                    plot_scores()\n",
    "\n",
    "                break\n",
    "        #if i_episode % TRAINSET_UPDATE == 0:\n",
    "        #    add_trainset()\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        if i_episode % 10000 == 0:\n",
    "            torch.save(policy_net.state_dict(), './data/net1/net_' + str(i_episode//10000))\n",
    "\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def net2_main(net1_name):\n",
    "    global episode_scores, episode_success\n",
    "    init_net(20, 32, 3000)\n",
    "    \n",
    "    reward_net = DQN(20, 12).to(device)\n",
    "    reward_net.load_state_dict(torch.load('./data/net1/' + net1_name))\n",
    "    reward_net.eval()\n",
    "    reward_net.requires_grad = False\n",
    "    \n",
    "    num_episodes = 500000\n",
    "    made_prob = 0.66\n",
    "    episode_scores = []\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        yacht.reset_game()\n",
    "        state, score, _, avail = yacht.get_yacht_output()\n",
    "        state = torch.tensor(state, dtype=torch.float, device=device, requires_grad = False)\n",
    "        for t in count():\n",
    "            avail = torch.tensor(avail, dtype=torch.float, device=device, requires_grad = False)\n",
    "            action = select_action(state.reshape(1,INPUT_SIZE), avail)\n",
    "            \n",
    "            if action >= 31:\n",
    "                a = policy_net(state)\n",
    "                a -= a.min()\n",
    "                action = (a * avail[31:]).max(1)[1].view(1, 1)\n",
    "            yacht.update(action)\n",
    "\n",
    "            new_state, _, done, avail = yacht.get_yacht_output()\n",
    "            step_reward = reward_net(state).max(1)[0]\n",
    "\n",
    "\n",
    "            if not done:\n",
    "                new_state = torch.tensor(new_state, dtype=torch.float, device=device, requires_grad = False)\n",
    "                memory.push(state.reshape(1,INPUT_SIZE), action, new_state.reshape(1,INPUT_SIZE), \\\n",
    "                        step_reward)\n",
    "            else:\n",
    "                new_state = None\n",
    "                memory.push(state.reshape(1,INPUT_SIZE), action, None, \\\n",
    "                        step_reward)\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "            optimize_model()\n",
    "            if done:\n",
    "                state, score, _, _ = yacht.get_yacht_output()\n",
    "                episode_scores.append(score)\n",
    "                if i_episode % 200 == 0:\n",
    "                    plot_scores()\n",
    "\n",
    "                break\n",
    "\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        if i_episode % 10000 == 0:\n",
    "            torch.save(policy_net.state_dict(), './data/net2/net_' + str(i_episode//10000))\n",
    "\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def net3_main(lr, g):\n",
    "    global episode_scores, episode_success, reset_net\n",
    "    init_net(20, 43, 10000, lr=lr, g=g,reset = reset_net)\n",
    "    reset_net = False\n",
    "    \n",
    "    num_episodes = 500000\n",
    "    num_average = 10\n",
    "    num_optimize = 10\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        episode_memory = []\n",
    "        episode_total = 0\n",
    "        episode_suctotal = 0\n",
    "        for i_average in range(num_average):\n",
    "            yacht.reset_game()\n",
    "            state, score, _, avail = yacht.get_yacht_output()\n",
    "            state = torch.tensor(state, dtype=torch.float, device=device, requires_grad = False)\n",
    "            for t in count():\n",
    "                avail = torch.tensor(avail, dtype=torch.float, device=device, requires_grad = False)\n",
    "                action = select_action(state.reshape(1,INPUT_SIZE), avail)\n",
    "\n",
    "                reward = yacht.update(action)\n",
    "                reward = torch.tensor([reward], device=device, requires_grad = False)\n",
    "\n",
    "                new_state, _, done, avail = yacht.get_yacht_output()\n",
    "\n",
    "                if not done:\n",
    "                    new_state = torch.tensor(new_state, dtype=torch.float, device=device, requires_grad = False)\n",
    "                    episode_memory.append(Transition(state.reshape(1,INPUT_SIZE), action, new_state.reshape(1,INPUT_SIZE), \\\n",
    "                            0))\n",
    "                    state = new_state\n",
    "                else:\n",
    "                    new_state = None\n",
    "                    if reward == -10:\n",
    "                        memory.push(state.reshape(1,INPUT_SIZE), action, None, reward)\n",
    "                    else:\n",
    "                        episode_memory.append(Transition(state.reshape(1,INPUT_SIZE), action, None, 0))                \n",
    "                    state, score, _, _ = yacht.get_yacht_output()\n",
    "                    episode_suctotal += state[:12].count(-1) * 10\n",
    "                    episode_total += score\n",
    "                    break;\n",
    "        \n",
    "        if state[:12].count(-1) < 12:\n",
    "            average_reward = episode_total / (12 - state[:12].count(-1))\n",
    "        else:\n",
    "            average_reward = 0\n",
    "        average_reward = torch.tensor([average_reward], device=device, requires_grad = False)\n",
    "        for x in episode_memory:\n",
    "            memory.push(x.state, x.action, x.next_state, average_reward)\n",
    "        \n",
    "        for i_average in range(num_optimize):\n",
    "            optimize_model()\n",
    "        \n",
    "        \n",
    "        episode_success.append(episode_suctotal/num_average)\n",
    "        episode_scores.append(episode_total/num_average)\n",
    "        if i_episode % 50 == 0:\n",
    "            plot_scores()\n",
    "        #if i_episode % TARGET_UPDATE == 0:\n",
    "        #    target_net.load_state_dict(policy_net.state_dict())\n",
    "        if i_episode % 1000 == 0:\n",
    "            torch.save(policy_net.state_dict(), './data/net3/net_' + str(i_episode//1000))\n",
    "\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = DQN(20, 12)\n",
    "net2 = DQN(20, 32)\n",
    "def duel_action(state):\n",
    "    action = net2(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(1, 45)).max(1)[1]\n",
    "\n",
    "    if action == 31:\n",
    "        action = net1(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(1, 45)).max(1)[1]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net1r,net2r):\n",
    "    net1 = DQN(20, 12)\n",
    "    net1.load_state_dict(torch.load('./data/net1/' + net1r))\n",
    "    net1.eval()\n",
    "    net2 = DQN(20, 32)\n",
    "    net2.load_state_dict(torch.load('./data/net2/' + net2r))\n",
    "    net2.eval()\n",
    "    \n",
    "    yacht.reset_game()\n",
    "    done = False\n",
    "    while not done :\n",
    "        state,reward,done,_ = yacht.get_yacht_output()\n",
    "        print(state)\n",
    "        action = duel_action(state)\n",
    "        yacht.update(action)\n",
    "        print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_net1(net1r):\n",
    "    net1 = DQN(20, 12)\n",
    "    net1.load_state_dict(torch.load('./data/net1/' + net1r))\n",
    "    net1.eval()\n",
    "    \n",
    "    yacht.reset_game()\n",
    "    done = False\n",
    "    while not done :\n",
    "        yacht.handled_roll()\n",
    "        state,reward,done,_ = yacht.get_yacht_output()\n",
    "        print(state)\n",
    "        action = net1(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(1, 45)).max(1)[1] + 31\n",
    "        yacht.update(action)\n",
    "        print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6f607ad0308d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet3_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.000001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-2cd45f44b885>\u001b[0m in \u001b[0;36mnet3_main\u001b[1;34m(lr, g)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mepisode_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_total\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_average\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mplot_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;31m#if i_episode % TARGET_UPDATE == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m#    target_net.load_state_dict(policy_net.state_dict())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-8ddece4de56a>\u001b[0m in \u001b[0;36mplot_scores\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpause\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mpause\u001b[1;34m(interval)\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mcanvas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstale\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1929\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1930\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_idle_draw_cntx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1931\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1933\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"3.2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    391\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[0;32m    392\u001b[0m               else nullcontext()):\n\u001b[1;32m--> 393\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m             \u001b[1;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;31m# don't forget to call the superclass.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[1;32m-> 1736\u001b[1;33m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'figure'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# Composite any adjacent images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer, inframe)\u001b[0m\n\u001b[0;32m   2588\u001b[0m                 \u001b[0martists\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_title_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxison\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minframe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_update_title_position\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   2536\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbb\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2537\u001b[0m                     \u001b[0mtop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mymax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2538\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_window_extent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mymin\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2539\u001b[0m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2540\u001b[0m                 \u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[1;34m(self, renderer, dpi)\u001b[0m\n\u001b[0;32m    903\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot get window extent w/o renderer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 905\u001b[1;33m         \u001b[0mbbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    906\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    291\u001b[0m         _, lp_h, lp_d = renderer.get_text_width_height_descent(\n\u001b[0;32m    292\u001b[0m             \u001b[1;34m\"lp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fontproperties\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m             ismath=\"TeX\" if self.get_usetex() else False)\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mmin_dy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlp_h\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlp_d\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_linespacing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[1;34m(self, s, prop, ismath)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0mflags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_hinting_flag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_agg_font\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[0mfont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_width_height\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# width and height of unrotated string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36m_get_agg_font\u001b[1;34m(self, prop)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0mGet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfont\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mefficiency\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \"\"\"\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindfont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_font\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\font_manager.py\u001b[0m in \u001b[0;36mfindfont\u001b[1;34m(self, prop, fontext, directory, fallback_to_default, rebuild_if_missing)\u001b[0m\n\u001b[0;32m   1223\u001b[0m         return self._findfont_cached(\n\u001b[0;32m   1224\u001b[0m             \u001b[0mprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfallback_to_default\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrebuild_if_missing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1225\u001b[1;33m             rc_params)\n\u001b[0m\u001b[0;32m   1226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\site-packages\\matplotlib\\font_manager.py\u001b[0m in \u001b[0;36m_findfont_cached\u001b[1;34m(self, prop, fontext, directory, fallback_to_default, rebuild_if_missing, rc_params)\u001b[0m\n\u001b[0;32m   1282\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_font\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1285\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrebuild_if_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m                 _log.info(\n",
      "\u001b[1;32mc:\\users\\roadmageb\\anaconda3\\envs\\d2l\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAXVklEQVR4nO3df5BlZX3n8ffHGUlASAAZcFB0lMgkmrJQG6JlcAHBAmJE3RQ/NrqTWlKsVvyBa7JhdbOSqk0VEghmC9cVIgIlDmiBSCkakBhZaxHswXH4LWpgHRhnmqCCQTHAd/+4p91rc3u6n+4+3T3M+1V1657zPM+55/vYJZ85v+5NVSFJ0mw9Y6kLkCTtWAwOSVITg0OS1MTgkCQ1MTgkSU0MDklSE4ND6kGSLyZZt9BjpeUgPschDST5ydDqbsBjwBPd+n+sqksXvypp+TE4pBGS3Av8cVV9eUTfyqp6fPGrkpYHT1VJM0hyeJLNSf48yQ+ATyTZK8nnk0wk+WG3/Lyhbf4xyR93y3+U5GtJzu7G/lOSY+c49oVJbkjySJIvJ/lIkk8u4v8cksEhzdJzgL2BFwCnMvj/zie69ecDPwXO2872vwPcDewDnAV8PEnmMPZTwM3As4EzgLfNeUbSHBkc0uw8CXywqh6rqp9W1T9X1RVV9WhVPQL8FfBvtrP9fVV1QVU9AVwMrAb2axmb5PnAIcB/q6qfV9XXgKsXaoLSbBkc0uxMVNXPJleS7JbkY0nuS/IwcAOwZ5IV02z/g8mFqnq0W9y9cez+wENDbQDfb5yHNG8GhzQ7U+8ieR+wFvidqvo14LVd+3SnnxbCFmDvJLsNtR3Q4/6kkQwOaW72YHBd40dJ9gY+2PcOq+o+YBw4I8kuSV4N/H7f+5WmMjikufkwsCvwIPB14EuLtN8/BF4N/DPw34HLGTxvAgyeRUlyWLd82PCzKUnen+SLi1SnnsZ8jkPagSW5HLirqno/4pEmecQh7UCSHJLkwCTPSHIMcDxw1VLXpZ3LyqUuQFKT5wBXMniOYzPwjqr65tKWpJ2Np6okSU08VSVJarJTnKraZ599as2aNUtdhiTtUDZs2PBgVa2a2r5TBMeaNWsYHx9f6jIkaYeS5L5R7Z6qkiQ1MTgkSU0MDklSE4NDktTE4JAkNTE4JElNDA5JUhODQ5LUxOCQJDUxOCRJTQwOSVITg0OS1MTgkCQ1MTgkSU0MDklSE4NDktTE4JAkNektOJJcmGRbktuG2i5PsrF73Ztk44jt1g6N2Zjk4SSndX1nJLl/qO+4vuqXJI3W50/HXgScB1wy2VBVJ04uJzkH+PHUjarqbuDgbswK4H7gs0NDzq2qs/spWZI0k96Co6puSLJmVF+SACcAR87wMa8DvltVI3/3VpK0+JbqGsdhwNaqumeGcScB66e0vTPJpu5U2F7TbZjk1CTjScYnJibmW68kqbNUwXEyTw2EX5JkF+CNwGeGmj8KHMjgVNYW4Jzptq+q86tqrKrGVq1aNf+KJUlAv9c4RkqyEngL8MoZhh4L3FJVWycbhpeTXAB8vpciJUnTWoojjqOAu6pq8wzjnnJUkmT10OqbgduQJC2qPm/HXQ/cCKxNsjnJKV3XU65bJNk/yTVD67sBRwNXTvnYs5LcmmQTcATw3r7qlySN1uddVSdP0/5HI9oeAI4bWn8UePaIcW9bwBIlSXPgk+OSpCYGhySpicEhSWpicEiSmhgckqQmBockqYnBIUlqYnBIkpoYHJKkJgaHJKmJwSFJamJwSJKaGBySpCYGhySpicEhSWpicEiSmhgckqQmBockqYnBIUlqYnBIkpqs7OuDk1wIvAHYVlW/3bVdDqzthuwJ/KiqDh6x7b3AI8ATwONVNda17w1cDqwB7gVOqKof9jUHSdJT9XnEcRFwzHBDVZ1YVQd3YXEFcOV2tj+iGzs21HY6cH1VvRi4vluXJC2i3oKjqm4AHhrVlyTACcD6xo89Hri4W74YeNOcC5QkzclSXeM4DNhaVfdM01/AtUk2JDl1qH2/qtoC0L3vO90OkpyaZDzJ+MTExIIVLkk7u6UKjpPZ/tHGa6rqFcCxwJ8keW3rDqrq/Koaq6qxVatWzbVOSdIUix4cSVYCb2FwkXukqnqge98GfBY4tOvammR19zmrgW39VitJmmopjjiOAu6qqs2jOpM8K8kek8vA64Hbuu6rgXXd8jrgcz3XKkmaorfgSLIeuBFYm2RzklO6rpOYcpoqyf5JrulW9wO+luRbwM3AF6rqS13fmcDRSe4Bju7WJUmLKFW11DX0bmxsrMbHx5e6DEnaoSTZMOWRCMAnxyVJjQwOSVITg0OS1MTgkCQ1MTgkSU0MDklSE4NDktTE4JAkNTE4JElNDA5JUhODQ5LUxOCQJDUxOCRJTQwOSVITg0OS1MTgkCQ1MTgkSU0MDklSE4NDktTE4JAkNektOJJcmGRbktuG2i5PsrF73Ztk44jtDkjylSR3Jrk9yXuG+s5Icv/QZxzXV/2SpNFW9vjZFwHnAZdMNlTViZPLSc4Bfjxiu8eB91XVLUn2ADYkua6q7uj6z62qs/srW5K0Pb0dcVTVDcBDo/qSBDgBWD9iuy1VdUu3/AhwJ/DcvuqUJLVZqmschwFbq+qe7Q1KsgZ4OXDTUPM7k2zqToXttZ1tT00ynmR8YmJiIWqWJLF0wXEyI442hiXZHbgCOK2qHu6aPwocCBwMbAHOmW77qjq/qsaqamzVqlULU7UkqddrHCMlWQm8BXjldsY8k0FoXFpVV062V9XWoTEXAJ/vsVRJ0ghLccRxFHBXVW0e1dld//g4cGdV/c2UvtVDq28GbkOStKj6vB13PXAjsDbJ5iSndF0nMeU0VZL9k1zTrb4GeBtw5Ijbbs9KcmuSTcARwHv7ql+SNFqqaqlr6N3Y2FiNj48vdRmStENJsqGqxqa2++S4JKmJwSFJamJwSJKaGBySpCYGhySpicEhSWpicEiSmhgckqQmBockqYnBIUlqYnBIkpoYHJKkJrMOjiS7JlnbZzGSpOVvVsGR5PeBjcCXuvWDk1zdZ2GSpOVptkccZwCHAj8CqKqNwJp+SpIkLWezDY7Hq+rHvVYiSdohzPY3x29L8u+AFUleDLwb+D/9lSVJWq5me8TxLuClwGPAp4AfA6f1VZQkafma8YgjyQrg6qo6CvhA/yVJkpazGY84quoJ4NEkv74I9UiSlrnZXuP4GXBrkuuAf5lsrKp3T7dBkguBNwDbquq3u7bLgclnQfYEflRVB4/Y9hjgb4EVwN9V1Zld+97A5Qzu6LoXOKGqfjjLOUiSFsBsr3F8AfgL4AZgw9Brey4CjhluqKoTq+rgLiyuAK6culF3auwjwLHAS4CTk7yk6z4duL6qXgxc361LkhbRrI44quriJLsAB3VNd1fVv86wzQ1J1ozqSxLgBODIEd2HAt+pqu91Yy8Djgfu6N4P78ZdDPwj8OezmYMkaWHM9snxw4F7GBwJ/E/g20leO4/9HgZsrap7RvQ9F/j+0Prmrg1gv6raAtC977udmk9NMp5kfGJiYh6lSpKGzfYaxznA66vqboAkBwHrgVfOcb8nd9uPkhFt1bqDqjofOB9gbGyseXtJ0mizDY5nToYGQFV9O8kz57LDJCuBtzB96GwGDhhafx7wQLe8NcnqqtqSZDWwbS41SJLmbrYXx8eTfDzJ4d3rAma+OD6do4C7qmrzNP3fAF6c5IXddZWTgMkvVLwaWNctrwM+N8caJElzNNvgeAdwO4OvGnkPgwvVb9/eBknWAzcCa5NsTnJK13USU05TJdk/yTUAVfU48E7g74E7gU9X1e3d0DOBo5PcAxzdrUuSFlGqZj79n+RZwM+6hwEnb5n9lap6tOf6FsTY2FiNj48vdRmStENJsqGqxqa2z/aI43pg16H1XYEvL0RhkqQdy2yD41er6ieTK93ybv2UJElazmYbHP+S5BWTK0nGgJ/2U5IkaTmb7e24pwGfSfIAg2cq9gdO7K0qSdKytd0jjiSHJHlOVX0D+E0GXzD4OIPfHv+nRahPkrTMzHSq6mPAz7vlVwPvZ/C1Iz+keypbkrRzmelU1YqqeqhbPhE4v6quAK5IsrHf0iRJy9FMRxwruq8IAXgd8A9DfbO9PiJJehqZ6T/+64GvJnmQwV1U/xsgyW8w+N1xSdJOZrvBUVV/leR6YDVwbf3/x8yfAbyr7+IkScvPjKebqurrI9q+3U85kqTlbrYPAEqSBBgckqRGBockqYnBIUlqYnBIkpoYHJKkJgaHJKmJwSFJamJwSJKa9BYcSS5Msi3JbVPa35Xk7iS3JzlrxHZrk2wcej2c5LSu74wk9w/1HddX/ZKk0fr8htuLgPOASyYbkhwBHA+8rKoeS7Lv1I2q6m7g4G78CuB+4LNDQ86tqrN7rFuStB29HXFU1Q3AQ1Oa3wGcWVWPdWO2zfAxrwO+W1X39VCiJGkOFvsax0HAYUluSvLVJIfMMP4kBl/tPuydSTZ1p8L2mm7DJKcmGU8yPjExMd+6JUmdxQ6OlcBewKuAPwM+nSSjBibZBXgj8Jmh5o8CBzI4lbUFOGe6HVXV+VU1VlVjq1atWqDyJUmLHRybgStr4GbgSWCfacYeC9xSVVsnG6pqa1U9UVVPAhcAh/ZesSTplyx2cFwFHAmQ5CBgF+DBacaezJTTVElWD62+GfilO7YkSf3r83bc9cCNwNokm5OcAlwIvKi7RfcyYF1VVZL9k1wztO1uwNHAlVM+9qwktybZBBwBvLev+iVJo/V2O25VnTxN11tHjH0AOG5o/VHg2SPGvW3BCpQkzYlPjkuSmhgckqQmBockqYnBIUlqYnBIkpoYHJKkJgaHJKmJwSFJamJwSJKaGBySpCYGhySpicEhSWpicEiSmhgckqQmBockqYnBIUlqYnBIkpoYHJKkJgaHJKmJwSFJatJbcCS5MMm2JLdNaX9XkruT3J7krGm2vTfJrUk2Jhkfat87yXVJ7une9+qrfknSaH0ecVwEHDPckOQI4HjgZVX1UuDs7Wx/RFUdXFVjQ22nA9dX1YuB67t1SdIi6i04quoG4KEpze8Azqyqx7ox2xo/9njg4m75YuBN8ypSktRssa9xHAQcluSmJF9Ncsg04wq4NsmGJKcOte9XVVsAuvd9p9tRklOTjCcZn5iYWLAJSNLObuUS7G8v4FXAIcCnk7yoqmrKuNdU1QNJ9gWuS3JXdwQza1V1PnA+wNjY2NTPlyTN0WIfcWwGrqyBm4EngX2mDqqqB7r3bcBngUO7rq1JVgN0762nuiRJ87TYwXEVcCRAkoOAXYAHhwckeVaSPSaXgdcDk3dmXQ2s65bXAZ9bhJolSUP6vB13PXAjsDbJ5iSnABcCL+pu0b0MWFdVlWT/JNd0m+4HfC3Jt4CbgS9U1Ze6vjOBo5PcAxzdrUuSFlGeennh6WdsbKzGx8dnHihJ+oUkG6Y8EgH45LgkqZHBIUlqYnBIkpoYHJKkJgaHJKmJwSFJamJwSJKaGBySpCYGhySpicEhSWpicEiSmhgckqQmBockqYnBIUlqYnBIkpoYHJKkJgaHJKmJwSFJamJwSJKaGBySpCa9BUeSC5NsS3LblPZ3Jbk7ye1Jzhqx3QFJvpLkzm7Me4b6zkhyf5KN3eu4vuqXJI22ssfPvgg4D7hksiHJEcDxwMuq6rEk+47Y7nHgfVV1S5I9gA1JrquqO7r+c6vq7B7rliRtR29HHFV1A/DQlOZ3AGdW1WPdmG0jtttSVbd0y48AdwLP7atOSVKbxb7GcRBwWJKbknw1ySHbG5xkDfBy4Kah5ncm2dSdCttrO9uemmQ8yfjExMRC1C5JYvGDYyWwF/Aq4M+ATyfJqIFJdgeuAE6rqoe75o8CBwIHA1uAc6bbUVWdX1VjVTW2atWqBZyCJO3cFjs4NgNX1sDNwJPAPlMHJXkmg9C4tKqunGyvqq1V9URVPQlcABy6SHVLkjqLHRxXAUcCJDkI2AV4cHhAdwTyceDOqvqbKX2rh1bfDPzSHVuSpP71eTvueuBGYG2SzUlOAS4EXtTdonsZsK6qKsn+Sa7pNn0N8DbgyBG33Z6V5NYkm4AjgPf2Vb8kabRU1VLX0LuxsbEaHx9f6jIkaYeSZENVjU1t98lxSVITg0OS1MTgkCQ1MTgkSU0MDklSE4NDktTE4JAkNTE4JElNDA5JUhODQ5LUxOCQJDUxOCRJTQwOSVITg0OS1MTgkCQ1MTgkSU12ih9ySjIB3LfUdczBPkz5ad2nuZ1tvuCcdxY76pxfUFWrpjbuFMGxo0oyPurXt56udrb5gnPeWTzd5uypKklSE4NDktTE4Fjezl/qAhbZzjZfcM47i6fVnL3GIUlq4hGHJKmJwSFJamJwLKEkeye5Lsk93fte04w7JsndSb6T5PQR/X+apJLs03/V8zPfOSf56yR3JdmU5LNJ9ly86tvM4u+WJP+j69+U5BWz3Xa5muuckxyQ5CtJ7kxye5L3LH71czOfv3PXvyLJN5N8fvGqnqeq8rVEL+As4PRu+XTgQyPGrAC+C7wI2AX4FvCSof4DgL9n8IDjPks9p77nDLweWNktf2jU9svhNdPfrRtzHPBFIMCrgJtmu+1yfM1zzquBV3TLewDffrrPeaj/PwGfAj6/1POZ7csjjqV1PHBxt3wx8KYRYw4FvlNV36uqnwOXddtNOhf4z8COcpfDvOZcVddW1ePduK8Dz+u53rma6e9Gt35JDXwd2DPJ6lluuxzNec5VtaWqbgGoqkeAO4HnLmbxczSfvzNJngf8HvB3i1n0fBkcS2u/qtoC0L3vO2LMc4HvD61v7tpI8kbg/qr6Vt+FLqB5zXmK/8DgX3LL0WzmMN2Y2c5/uZnPnH8hyRrg5cBNC17hwpvvnD/M4B9+T/ZVYB9WLnUBT3dJvgw8Z0TXB2b7ESPaKslu3We8fq619aWvOU/ZxweAx4FL26pbNDPOYTtjZrPtcjSfOQ86k92BK4DTqurhBaytL3Oec5I3ANuqakOSwxe8sh4ZHD2rqqOm60uydfIwvTt03TZi2GYG1zEmPQ94ADgQeCHwrSST7bckObSqfrBgE5iDHuc8+RnrgDcAr6vuJPEytN05zDBml1lsuxzNZ84keSaD0Li0qq7ssc6FNJ85/wHwxiTHAb8K/FqST1bVW3usd2Es9UWWnfkF/DW/fKH4rBFjVgLfYxASkxffXjpi3L3sGBfH5zVn4BjgDmDVUs9lhnnO+HdjcG57+KLpzS1/8+X2muecA1wCfHip57FYc54y5nB2oIvjS17AzvwCng1cD9zTve/dte8PXDM07jgGd5l8F/jANJ+1owTHvOYMfIfB+eKN3et/LfWctjPXp8wBeDvw9m45wEe6/luBsZa/+XJ8zXXOwO8yOMWzaehve9xSz6fvv/PQZ+xQweFXjkiSmnhXlSSpicEhSWpicEiSmhgckqQmBockqYnBIc1BkieSbBx6bfcbbJO8Pcm/X4D93rsjfAuynt68HVeagyQ/qardl2C/9zJ4DuDBxd63NMkjDmkBdUcEH0pyc/f6ja79jCR/2i2/O8kd3W8zXNa17Z3kqq7t60le1rU/O8m13e81fIyh7z1K8tZuHxuTfCzJiiWYsnZCBoc0N7tOOVV14lDfw1V1KHAeg28/nep04OVV9TIGTxgD/CXwza7t/Qy+fgPgg8DXqurlwNXA8wGS/BZwIvCaqjoYeAL4w4WdojSaX3Iozc1Pu/9gj7J+6P3cEf2bgEuTXAVc1bX9LvBvAarqH7ojjV8HXgu8pWv/QpIfduNfB7wS+Eb3JZe7MvoLI6UFZ3BIC6+mWZ70ewwC4Y3AXyR5Kdv/eu5RnxHg4qr6L/MpVJoLT1VJC+/EofcbhzuSPAM4oKq+wuAHfPYEdgduoDvV1P02w4M1+D2K4fZjgcnfaL8e+IMk+3Z9eyd5QY9zkn7BIw5pbnZNsnFo/UtVNXlL7q8kuYnBP8xOnrLdCuCT3WmoAOdW1Y+SnAF8Iskm4FFgXTf+L4H1SW4Bvgr8X4CquiPJfwWu7cLoX4E/YfDb81KvvB1XWkDeLqudgaeqJElNPOKQJDXxiEOS1MTgkCQ1MTgkSU0MDklSE4NDktTk/wEp5YNMM6/H/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net3_main(0.000001, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net1_main(0.0001, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_END = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2_main('net_99')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model('net_', 'net_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_net1('net_49')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (diceonly): Linear(in_features=20, out_features=43, bias=True)\n",
       "  (dice1): Linear(in_features=20, out_features=300, bias=True)\n",
       "  (dice2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (dice3): Linear(in_features=300, out_features=50, bias=True)\n",
       "  (diceconv): Conv2d(1, 300, kernel_size=(1, 20), stride=(1, 1))\n",
       "  (convshape): Reshape()\n",
       "  (activation): LeakyReLU(negative_slope=0.1)\n",
       "  (dice7): Linear(in_features=50, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1 = DQN(20, 43)\n",
    "net1.load_state_dict(torch.load('./data/score_per_action_net/net2'))\n",
    "net1.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alarm\n",
      "alarm\n",
      "alarm\n",
      "alarm\n",
      "alarm\n",
      "alarm\n",
      "alarm\n",
      "alarm\n",
      "alarm\n",
      "alarm\n",
      "Final:116.4605\n",
      "Fail:0.0\n",
      "FailNum:0\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "total_action = 0\n",
    "total_fail = 0\n",
    "for i in range(10000):\n",
    "    yacht.reset_game()\n",
    "    state = []\n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done :\n",
    "        state,score,done,avail = yacht.get_yacht_output()\n",
    "        #print(state)\n",
    "        action_value = net1(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(1, 20))\n",
    "        avail = torch.tensor(avail, dtype=torch.float, requires_grad = False)\n",
    "        real_action = action_value.max(1)[1]\n",
    "        avail_action = ((action_value - action_value.min()) * avail).max(1)[1]\n",
    "        if real_action != avail_action:\n",
    "            total_fail+=1\n",
    "        reward = yacht.update(avail_action)\n",
    "        total_action+=1\n",
    "    total += score\n",
    "    if i % 1000 == 0:\n",
    "        print('alarm')\n",
    "    #print(score)\n",
    "print('Final:' + str(total / 10000))\n",
    "print('Fail:' + str(total_fail / total_action))\n",
    "print('FailNum:' + str(total_fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yacht.reset_game()\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yacht.dice_status = [5, 5, 5, 5, 5]\n",
    "state,reward,done,_ = yacht.get_yacht_output()\n",
    "print(state)\n",
    "print(net1(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(-1)))\n",
    "action = net1(torch.tensor(state, dtype=torch.float, requires_grad = False).reshape(1, 45)).max(1)[1] + 31\n",
    "reward = yacht.update(action)\n",
    "print(str(action-30) + ' ' + str(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), './data/score_per_action_net/net2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_net = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
